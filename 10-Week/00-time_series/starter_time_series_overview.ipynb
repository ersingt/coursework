{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaching Time Series Forecasting Problems ðŸ—“\n",
    "\n",
    "The goal of this lesson is to help students think through how to approach a time series problem.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of the lesson students should be familiar with:\n",
    "\n",
    "- Holt-Winters Triple Exponential Smoothing\n",
    "- SARIMAX\n",
    "- auto-ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before modeling\n",
    "- Get the time period into *datetime* dtype format and get it in the index\n",
    "- Become comfortable with resampling and other pandas time series methods\n",
    "\n",
    "## Modeling considerations\n",
    "- Make sure you aren't leaking information from the future into your training data\n",
    "- Make a null model (just the most recent prediction continued) first, so you have a baseline\n",
    "- There are a lot of model possibilities of various complexity levels (e.g. weighted average, simple exponential smoothing, ARIMA)\n",
    "- Consider the tradeoffs between complexity and performance when choosing which model to use\n",
    "\n",
    "## Models that can handle more complexity well\n",
    "- Holt-Winters (exponential smoothing)\n",
    "- Prophet \n",
    "- SARIMAX \n",
    "- VAR\n",
    "- Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Holt-Winters (exponential smoothing)\n",
    "[Holt Winters](https://www.statsmodels.org/dev/generated/statsmodels.tsa.holtwinters.ExponentialSmoothing.html) often performs about as well as SARIMA. It requires much less tuning. It's part of the exponential smoothing family of algorithms. \n",
    "\n",
    "- It's also called *Triple Exponential Smoothing*\n",
    "\n",
    "- You don't have to worry about stationarity. \n",
    "\n",
    "- It's available in statsmodels [here](https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.exponential_smoothing.ExponentialSmoothing.html#statsmodels.tsa.statespace.exponential_smoothing.ExponentialSmoothing). \n",
    "\n",
    "Simple exponential smoothing uses weighted averages where the weights decrease exponentially for older data points.\n",
    "\n",
    "Holt-Winters is more powerful because it accounts for seasonality and trend. \n",
    "\n",
    "The model is made up of trend, level, and seasonal components.\n",
    "\n",
    "If the outcome variable is sales:\n",
    "\n",
    "* The trend equation captures the overall direction of sales. \n",
    "* The level equation is a weighted average between the seasonally adjusted observation and the non-seasonal forecast for time `t`. \n",
    "* The seasonal equation is a weighted average between the current seasonal index and the seasonal index of the same season `s` time periods ago. \n",
    "\n",
    "You have to choose the number of seasonal periods, and whether there are additive,  multiplicative, or damping.\n",
    "\n",
    "You can also pass exogenous variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's what it looks like in code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statsmodels.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "holt_winters = ExponentialSmoothing(\n",
    "    (train['Weekly_Sales'].to_numpy()),  # note have to feed it numpy ndarray\n",
    "    trend='mul', \n",
    "    seasonal='mul',\n",
    "    seasonal_periods=52,\n",
    "\n",
    ").fit()\n",
    "\n",
    "df_predictions['Holt_Winters'] = holt_winters.forecast(len(test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:43:15.129378Z",
     "start_time": "2019-09-09T13:43:14.632706Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot( train['Weekly_Sales'], label='Train')\n",
    "plt.plot(test['Weekly_Sales'], label='Test')\n",
    "plt.plot(df_predictions['Holt_Winters'], label='Holt_Winter')\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Holt Winters!\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think of that curve?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the RMSE to check accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:43:17.406189Z",
     "start_time": "2019-09-09T13:43:17.400205Z"
    }
   },
   "outputs": [],
   "source": [
    "rms = mean_squared_error(\n",
    "    test['Weekly_Sales'], \n",
    "    df_predictions['Holt_Winters'], \n",
    "    squared=False\n",
    ")\n",
    "print(f'{rms:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet\n",
    "Facebook created and opensourced the [Prophet](https://facebook.github.io/prophet/) library. \n",
    "\n",
    "[Here's](https://medium.com/future-vision/the-math-of-prophet-46864fa9c55a) a good article on what Prophet is doing.\n",
    "\"Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects.\" \n",
    "\n",
    "An additive model is a nonparametric regression method.\n",
    "\n",
    "Prophet requires much less tuning than SARIMA. Facebook finds it very helpful for advertising type data and it's convenient for adding things like holidays. It's relatively new and it's popular. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMAX\n",
    "You know ARIMA. \n",
    "\n",
    "SARIMA adds seasonality (and you get to choose 4 other parameters) ðŸ˜€. \n",
    "\n",
    "There are 7 parameters to choose `(p, d, q)` `(P, D, Q)` and `m`. `p` is for the autoregressive component, `i` stands for *integrative* and is all about differencing, `q` is for the moving average component. The next three capitalized arguments are the same but for the Seasonality component. And `m` is for the number of observations per seasonal cylce. With weekly sales data with a holiday sesonality, that's would be 52 weeks, generally.\n",
    "\n",
    "SARIMAX extends SARIMA. The X in [SARIMAX](https://www.statsmodels.org/dev/examples/notebooks/generated/statespace_sarimax_stata.html) stands for eXogenous variables. It allows you to add predictors that aren't just the prevoious outcome variable. For example, you can dummy encode a column to be a holiday/no holiday, (1/0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VAR\n",
    "\n",
    "VAR is useful when you have multiple time series you want to include in your forecast.\n",
    "\n",
    "VAR was touched on briefly in global and Mahdi has a whole repo [here](https://git.generalassemb.ly/DSI-US-11/local_var)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "Deep neural networks can be used with time series data. There are a number of of architectures that can make sense. [Here's](https://towardsdatascience.com/deep-learning-for-time-series-and-why-deep-learning-a6120b147d60) a post on the topic if you are interested. If the relationship between the input variables and the outcome is not complex, this is probably overkill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "If you find a model that works really well, great!\n",
    "\n",
    "If you are trying to get every last bit of performance, you can also ensemble your models. I have not done this.\n",
    "\n",
    "## Next steps\n",
    "I recommend checking out the great free ebook [Forecasting: Principles and Practice\n",
    "Rob J Hyndman and George Athanasopoulos](https://otexts.com/fpp3/) to learn more about time series. \n",
    "\n",
    "Jason Brownlee aslo has a lot of great information and Python code on time series forecasting at [Machine Learning Mastery](https://machinelearningmastery.com/exponential-smoothing-for-time-series-forecasting-in-python/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Store sales data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the dataset, set the index and take a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:15.573361Z",
     "start_time": "2019-09-09T13:42:15.000892Z"
    }
   },
   "outputs": [],
   "source": [
    "df_walmart = pd.read_csv('data/train.csv', ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could have set the index after creating the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:15.573361Z",
     "start_time": "2019-09-09T13:42:15.000892Z"
    }
   },
   "outputs": [],
   "source": [
    "# df_walmart = df_walmart.set_index('Date') \n",
    "df_walmart.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make sure that index is a `datetime`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter the DataFrame to Store 1 sales and aggregate over departments to compute the total sales per store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:17.287774Z",
     "start_time": "2019-09-09T13:42:17.262841Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:17.287774Z",
     "start_time": "2019-09-09T13:42:17.262841Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store1_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make that a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store1_sales = pd.DataFrame(store1_sales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store1_sales.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_store1_sales.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:21.079629Z",
     "start_time": "2019-09-09T13:42:20.002510Z"
    }
   },
   "outputs": [],
   "source": [
    "df_store1_sales.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will subset the data to look at the accuracy of each time series forecasting method on the data set. We'll use the first two years (2010â€“2011) as the \"training\" data and the last year (2012) as the \"testing\" data for the purposes of our demonstration. \n",
    "\n",
    "Let's make two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:23.025423Z",
     "start_time": "2019-09-09T13:42:23.017445Z"
    }
   },
   "outputs": [],
   "source": [
    "train = df_store1_sales['2010': '2011']\n",
    "test = df_store1_sales['2012']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the two DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:25.897738Z",
     "start_time": "2019-09-09T13:42:24.856525Z"
    }
   },
   "outputs": [],
   "source": [
    "train['Weekly_Sales'].plot(figsize=(15,8))\n",
    "test['Weekly_Sales'].plot(figsize=(15,8), title= 'Weekly Sales', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1: Naive Forecast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the last time period's sales and estimate the same value for all future time periods. This method is called a **naive forecast**.\n",
    "\n",
    "$${\\Large \\hat y_{t+1} = y_t}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's see how well the naive method does when forecasting sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a predictions DataFrame and set the predicted values equal to the last value in the `Weekly_Sales` df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:28.230496Z",
     "start_time": "2019-09-09T13:42:27.565278Z"
    }
   },
   "outputs": [],
   "source": [
    "df_predictions = test.copy()  \n",
    "df_predictions['naive'] = train['Weekly_Sales'].iloc[-1]\n",
    "df_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the historic data, the actual \"future\" data, and the predicted \"future\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:28.230496Z",
     "start_time": "2019-09-09T13:42:27.565278Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.plot(train.index, train['Weekly_Sales'], label='Train')\n",
    "plt.plot(test.index, test['Weekly_Sales'], label='Test')\n",
    "plt.plot(df_predictions.index, df_predictions['naive'], label='Naive Forecast')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.title(\"Naive Forecast\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the model's performance on the test data set using the Root Mean Squared Error (RMSE) scoring metric. \n",
    "\n",
    "This `mean_squared_error metric` function can be imported from Scikit-learn (sklearn). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:34.877712Z",
     "start_time": "2019-09-09T13:42:33.263033Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the RMSE for our baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-09T13:42:34.877712Z",
     "start_time": "2019-09-09T13:42:33.263033Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's our baseline. \n",
    "\n",
    "We can infer from the RMSE value and the graph above that the naive method isnâ€™t amazing for data sets with high variability. It's best suited for stable data sets. Let's try to improve the score with more sophisticated models. \n",
    "\n",
    "Note that we're looking at a prediction through the end of the time period as of a single date, we aren't assuming we're updating the model every week with new information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SARIMA with Auto-arima package: pmdarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a bunch of ways to find parameter values for SARIMA. You can use autocorrelation plots and partial correlation plots to try to figure out parameters, but this is inexact and tedious. You can make your own GridSearching function. Or you, can use a nice one out of the box.\n",
    "\n",
    "Let's do some statistical testing and search through parameters automatically the `pdarima.auto_arima` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Auto-arima algorithm can grid search for `p` and `q`. It doesn't help with `d`. So we'll figure out the differencing hyperparameter ourselves.\n",
    "\n",
    "If you want to use this library in the future, install with pip because the conda instructions didn't work as of 2/1/20.\n",
    "\n",
    "From the command line run: \n",
    "`pip install pmdarima`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I imported the pmdarima package under the alias `pm`. It has some helpful functions and will pick parameters and create a model using statsmodels under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pmd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make an autocorrelation plot just for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test for what the differencing coefficient (d) should be. Instructions from https://alkaline-ml.com/pmdarima/tips_and_tricks.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndiffs = pmd.arima.utils.ndiffs   \n",
    "\n",
    "# Estimate the number of differences using an ADF (Augmented Dickey Fuller) test:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No differencing needed. Nice. ðŸ˜€"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`m=52` for the number of observations per seasonal cycle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate a model with a whole lot of arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running this next cell might take some time. The `summary)` method is gridsearching, fitting lots of different models to the training data to see which hyperparameters work best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Model displays the final parameters. Now we can the model object to predict the \"future\" values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,8))\n",
    "plt.plot( train['Weekly_Sales'], label='Train')\n",
    "plt.plot(test['Weekly_Sales'], label='Test')\n",
    "plt.plot(df_predictions['SARIMA_auto'], label='SARIMA_auto')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms = mean_squared_error(\n",
    "    test['Weekly_Sales'], \n",
    "    df_predictions['SARIMA_auto'],\n",
    "    squared=False\n",
    ")\n",
    "print(f'{rms:,.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That's all the time we have today! ðŸ˜‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
