{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Quiz 5 In-Class Review\n",
    "\n",
    " _**Author:** Boom D. (DSI-NYC); **Edited:** Adi Bronshtein (DSI-DC)_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** What are the two ways we can train models? In other words, what are the two types of learning and what distinguishes them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised Learning\n",
    "- Has a target variable that we're trying to predict. \n",
    "\n",
    "Unsupervised Learning\n",
    "- Doesn't have a target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Give two examples of learning algorithms that solely require just our independent variables / features / X's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- KMeans \n",
    "- DBSCAN\n",
    "- hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** In $k$-means clustering, what do we mean by $k$ and \"means\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k = number of clusters\n",
    "means - means of the clusters; average distance of all of the points from the centroid point (of the cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Using the whiteboard, briefly outline how the $k$-means clustering algorithm works. (_Be sure to identify what hyperparameters must be specified by the user and use any appropriate model-specific terminology_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of k is chosen which will dictate the number of centroids. The centroids are randomly placed. Then the average distance to the centroid is calculated. The average distance is then adjusted to create a better average to each point.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Inertia is one sort of evaluation metric for $k$-means clustering algorithm that is calculated as the sum of squared errors of each point from its cluster's centroid. If $k = n$ (sample size), what would you expect the value of inertia for this clustering algorithm to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0, every data point will be a clusters, and clusters will match perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** You're a data scientist at AdiData and you have an [intern named James](https://twitter.com/k_alex_h/status/1020056105492369409?lang=en). James is helping you build a $k$-means clustering algorithm on hedge fund data as a possible way to categorize individual hedge funds by type: Event Driven, Global Macro, Market Neutral, etc. _(If you're interested in learning more about this, [check this out](http://depts.washington.edu/sce2003/Papers/284.pdf))_\n",
    "\n",
    "Suppose you have 100 hedge funds in your sample. James proposes setting $k = 93$. Explain why that choice of $k$ is or isn't appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We almost get a ratio of 1-1 (93 clusters and 100 samples) - would be very hard to create distinct clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Another evaluation metric for $k$-means clustering algorithms is the **[Silhouette](https://youtu.be/6VJBBUqr1wM?t=183) Score**. This score considers two kinds of distances. What are they?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cohesion** - the average distance between the centroid and all of the data points in that **cluster**. \n",
    "\n",
    "**Sepration** - the average distance of all points in one cluster to the data points in another cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** You scold James the intern from Q6 for building a bad model and ask him to go back and think about how their next attempt at a model does in terms of the Silhoutte Score. He/she comes back with a new model and excitedly tells you their Silhoutte Score is -0.89. Should you give James a return offer? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a pretty bad score! Poor performance of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9**. Briefly explain why it is necessary to scale data as a preprocessing step before any clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clustering based on distance; different scales will tip the results of our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** What is one major pitfall of $k$-means clustering that DBSCAN clustering potentially addresses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In K-Means, in order to get the best results, we need to know (or have an idea) of the number of clusters. DBSCAN clusters data points together without a pre-defined number of clusters. DBSCAN accounts for outliers (while K-Means - not so much...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** DBSCAN requires you to specify two hyperparameters. Identify them and briefly discuss their roles in the DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epsilon (distance between the data points).\n",
    "\n",
    "Min samples - how many data points we want in each clusters (the minimum number of data points required to form a cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** You have a subscription for HBO Now. The site notices you liked watching Game of Thrones and searching through its list of available shows. Briefly explain one metric that could be used to determine how similar two movies are. Be sure to identify what the possible range of values for that metric are as well as how to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User (Collaborative) based recommender - who else watch GOT and likes shows that are similar to GOT.  \n",
    "Content based recommender - What is the content of GOT that's similar to other shows you're getting recommendations for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15.** Dimensionality Reduction methods can be categorized as **Feature Elimination** or **Feature Extraction**. Explain the difference between these two and provide one example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Elimination** - dropping features from the dataset (or not using them in our predictive model[s])  \n",
    "**Feature Extraction** - combining features to minimize/reduce the number of features, without losing **too much** data (pulling the most important aspects of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16.** Describe a scenario where PCA may be more appropriate than regularization techniques that tone down the effect of coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When there's a large spread of data - a very large number of features and a large spread between data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17.** One of the assumptions of PCA is linearity, i.e. PCA detects and controls for linear relationships, so we assume that the data does not hold nonlinear relationships (or that we don't care about these nonlinear relationships). What is the other core assumption of the algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Large variance between the features.  \n",
    "Correlation - the features should somewhat correlated to each other.  \n",
    "Outliers - PCA is pretty sensitive to outliers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q18.** Briefly explain what the goal of the PCA algorithm is and how it works. _(You do not need to refer to the spectral decomposition formula as this is not a memorization exercise. However, you should be able to conceptually explain how eigenvectors and eigenvalues come into play)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Explain Like I'm 5 - PCA](https://www.reddit.com/r/explainlikeimfive/comments/17xk21/eli5_principle_component_analysis_pcn/c89rmai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q19.** Suppose you run the following code:\n",
    "\n",
    "```python\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "Z_train = pca.transform(X_train)\n",
    "Z_test = pca.transform(X_test)\n",
    "```\n",
    "\n",
    "Doing Shift+Tab on `PCA()` will reveal `n_components = None`. However, this isn't zero. What is implied to be the default number of components the algorithm will use if we don't specify `n_components`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA, by default would give us all of the features in our features matrix **OR** the number of samples (the smaller between the two)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20.** Suppose you have a data set with 4 features. You find that the eigenvalues are $\\lambda_1 = 38$, $\\lambda_2 = 7$, and $\\lambda_3 = 3$, $\\lambda_4 = 2$. If you wanted to constructed a model that captures a minimum of 90% of the total variance, what should you set `n_components` to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sum of variance is 50 ($38+7+3+2$).\n",
    "$90%$ of 50 is $45$. **2 components** -  $\\lambda_1 = 38$ and $\\lambda_2 = 7$ will give 45. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20.** Discuss one major issue when it comes to imputing missing data using any method that doesn't involve going out to re-collect data for missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making up data.  \n",
    "We might skew the data - variance, standard deviation, change the mean or median."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q21.** What are the various methods we learned for imputation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inferential imputation - based on inferences and/or intuition.  \n",
    "Dedactive - based on other data points, patterns, trends in the data, etc.\n",
    "We can impute based the mean, median, mode (for categorical data).\n",
    "Use regression to estimate/predit values.\n",
    "Fill in with 0's or high negative value (use case based)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q22.** Discuss an example of a case when imputing the mean is not appropriate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Messes up the standard deviation.  \n",
    "Large number of null values (shifts the distribution towards the mean).  \n",
    "Mean is the measure of central tendency that's the most sensitive to outliers.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q23.** What are some of the main problems with the way many of us have imputed missing data (mean, median, or mode) throughout the course?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It skews our data (and/or results).  \n",
    "We're \"making up\" data.  \n",
    "We're underestimating the variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.giphy.com/media/uFmMNcXaDNrMCzjCMq/giphy.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://media.giphy.com/media/uFmMNcXaDNrMCzjCMq/giphy.gif')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://media.giphy.com/media/12XDYvMJNcmLgQ/giphy.gif\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='https://media.giphy.com/media/12XDYvMJNcmLgQ/giphy.gif')  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
