{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 15px; height: 80px\">\n",
    "\n",
    "# Quiz 5 In-Class Review\n",
    "\n",
    " _**Author:** Boom D. (DSI-NYC); **Edited:** Adi Bronshtein (DSI-DC)_\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** What are the two ways we can train models? In other words, what are the two types of learning and what distinguishes them?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** Give two examples of learning algorithms that solely require just our independent variables / features / X's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** In $k$-means clustering, what do we mean by $k$ and \"means\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Using the whiteboard, briefly outline how the $k$-means clustering algorithm works. (_Be sure to identify what hyperparameters must be specified by the user and use any appropriate model-specific terminology_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** Inertia is one sort of evaluation metric for $k$-means clustering algorithm that is calculated as the sum of squared errors of each point from its cluster's centroid. If $k = n$ (sample size), what would you expect the value of inertia for this clustering algorithm to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** You're a data scientist at AdiData and you have an [intern named James](https://twitter.com/k_alex_h/status/1020056105492369409?lang=en). James is helping you build a $k$-means clustering algorithm on hedge fund data as a possible way to categorize individual hedge funds by type: Event Driven, Global Macro, Market Neutral, etc. _(If you're interested in learning more about this, [check this out](http://depts.washington.edu/sce2003/Papers/284.pdf))_\n",
    "\n",
    "Suppose you have 100 hedge funds in your sample. James proposes setting $k = 93$. Explain why that choice of $k$ is or isn't appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** Another evaluation metric for $k$-means clustering algorithms is the **[Silhouette](https://youtu.be/6VJBBUqr1wM?t=183) Score**. This score considers two kinds of distances. What are they?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** You scold James the intern from Q6 for building a bad model and ask him to go back and think about how their next attempt at a model does in terms of the Silhoutte Score. He/she comes back with a new model and excitedly tells you their Silhoutte Score is -0.89. Should you give James a return offer? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q9**. Briefly explain why it is necessary to scale data as a preprocessing step before any clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q10.** What is one major pitfall of $k$-means clustering that DBSCAN clustering potentially addresses?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q11.** DBSCAN requires you to specify two hyperparameters. Identify them and briefly discuss their roles in the DBSCAN clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q12.** You have a subscription for HBO Now. The site notices you liked watching Game of Thrones and searching through its list of available shows. Briefly explain one metric that could be used to determine how similar two movies are. Be sure to identify what the possible range of values for that metric are as well as how to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q13.** Dimensionality Reduction methods can be categorized as **Feature Elimination** or **Feature Extraction**. Explain the difference between these two and provide one example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q14.** Describe a scenario where PCA may be more appropriate than regularization techniques that tone down the effect of coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q15.** One of the assumptions of PCA is linearity, i.e. PCA detects and controls for linear relationships, so we assume that the data does not hold nonlinear relationships (or that we don't care about these nonlinear relationships). What is the other core assumption of the algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q16.** Briefly explain what the goal of the PCA algorithm is and how it works. _(You do not need to refer to the spectral decomposition formula as this is not a memorization exercise. However, you should be able to conceptually explain how eigenvectors and eigenvalues come into play)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q17.** Suppose you run the following code:\n",
    "\n",
    "```python\n",
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "Z_train = pca.transform(X_train)\n",
    "Z_test = pca.transform(X_test)\n",
    "```\n",
    "\n",
    "Doing Shift+Tab on `PCA()` will reveal `n_components = None`. However, this isn't zero. What is implied to be the default number of components the algorithm will use if we don't specify `n_components`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q19.** Suppose you have a data set with 4 features. You find that the eigenvalues are $\\lambda_1 = 38$, $\\lambda_2 = 7$, and $\\lambda_3 = 3$, $\\lambda_4 = 2$. If you wanted to constructed a model that captures a minimum of 90% of the total variance, what should you set `n_components` to?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q20.** Discuss one major issue when it comes to imputing missing data using any method that doesn't involve going out to re-collect data for missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q21.** Discuss an example of a case when imputing the mean is not appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the various methods we learned for imputation? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are some of the main problems with the way many of us have imputed missing data (mean, median, or mode) throughout the course?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
