{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Fun 🎉 \n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "By Jeff Hale\n",
    "\n",
    "\n",
    "### Learning Objectives:\n",
    "\n",
    "By the end of this lesson students will:\n",
    "- Have learned a workflow for using text data in models\n",
    "- Understand sklearn's CountVectorizer and TfidfVectorizer\n",
    "- Use nltk's lemmatization or stemming as part of CountVectorizer or TfidfVectorizer\n",
    "- Use CountVectorizer and TfidfVectorizer in a Pipeline with GridSearchCV\n",
    "- Be able to use make_column_transformer to create pipelines with text and non-text features\n",
    "\n",
    "\n",
    "When you have text data and you want to make a model this is the workflow I suggest:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a basic model first\n",
    "\n",
    "Just use the text data\n",
    "- Use CountVectorizer to transform\n",
    "- Use MultinomialNB to predict\n",
    "\n",
    "Now you have a baseline model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then add complexity\n",
    "\n",
    "### Options:\n",
    "- Add lemmatization/stemming\n",
    "- Hyperparameter tune (e.g. ngrams)\n",
    "- Use Tfidf\n",
    "- Add non-text features\n",
    "\n",
    "Put your workflow a Pipeline with GridSearchCV to be able to iterate faster and reduce the chance of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "The data is Yelp reviews.\n",
    "\n",
    "We'll filter the data so it just includes the 5-star and 1-star responses.\n",
    "\n",
    "The goal is to classify a reviews number of stars as a 5 or a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './data/yelp.csv'\n",
    "yelp = pd.read_csv(path)\n",
    "yelp.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter the DataFrame to only keep reviews with 1 or 5 stars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5    3337\n",
       "1     749\n",
       "Name: stars, dtype: int64"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp_best_worst = yelp[~yelp['stars'].between(2,4)]\n",
    "yelp_best_worst['stars'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split into X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_best_worst['text']\n",
    "y = yelp_best_worst['stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split into train and test"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:05-Week/05-nlp/NLP_solution.ipynb
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CountVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9552acc64abb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_cvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test_cvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "X_train_cvec = cv.fit_transform(X_train)\n",
    "X_test_cvec = cv.transform(X_test)"
=======
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2157    I just started taking my car here a few months...\n",
       "3342    HELLLLLLLLO GOLFERS!   What a sexy course this...\n",
       "7768    We saw a small door with a sign over it and we...\n",
       "3182    One word to describe Over Easy: disappointing....\n",
       "1527    The portions are too small, the plasticware ju...\n",
       "                              ...                        \n",
       "4275    I've been here three times and have had terrif...\n",
       "6349    I LOVE this bar. My first visit was in 2004 wh...\n",
       "7254    Dr. Ellis is compasionate, caring, and really ...\n",
       "8596    Nice facilities, nice AC, but two FATAL flaws:...\n",
       "4007    \"Wait, we're in Arizona?\"\\n\" No seriously, thi...\n",
       "Name: text, Length: 3064, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, stratify=y)\n",
    "X_train"
>>>>>>> 10ab787919ea6967db9735bab54ef957e9363257:05-Week/05-nlp/NLP_starter_post_class_jeff.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic CountVectorizer first\n",
    "\n",
    "Use CountVectorizer to process the text data.\n",
    "\n",
    "- use the built-in stopwords\n",
    "- lowercase everything\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x16700 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 372 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
<<<<<<< HEAD:05-Week/05-nlp/NLP_solution.ipynb
    "nb = MultinomialNB() \n",
    "\n",
    "nb.fit(X_train_processed, y_train)\n",
    "nb.score(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did that do?"
=======
    "cvec = CountVectorizer()\n",
    "X_train_cvec= cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)"
>>>>>>> 10ab787919ea6967db9735bab54ef957e9363257:05-Week/05-nlp/NLP_starter_post_class_jeff.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a Naive Bayes\n",
    "\n",
    "Instantiate, fit_transform, transform, and predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9774804177545692"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
<<<<<<< HEAD:05-Week/05-nlp/NLP_solution.ipynb
    "nb.fit(X_train_processed, y_train)\n",
    "nb.score(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TFIDF\n",
    "\n",
    "Now try a TFIDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TfidfVectorizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f14326f5a282>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mX_train_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_test_processed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
     ]
    }
   ],
   "source": [
    "cv = TfidfVectorizer()\n",
    "X_train_processed = cv.fit_transform(X_train)\n",
    "X_test_processed = cv.transform(X_test)"
=======
    "model = nb.fit(X_train_cvec, y_train)\n",
    "model.score(X_train_cvec, y_train)"
>>>>>>> 10ab787919ea6967db9735bab54ef957e9363257:05-Week/05-nlp/NLP_starter_post_class_jeff.ipynb
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD:05-Week/05-nlp/NLP_solution.ipynb
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'MultinomialNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-247f64252c5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_processed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'MultinomialNB' is not defined"
     ]
=======
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9148727984344422"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
>>>>>>> 10ab787919ea6967db9735bab54ef957e9363257:05-Week/05-nlp/NLP_starter_post_class_jeff.ipynb
    }
   ],
   "source": [
    "model.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How did that do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's add stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvec = CountVectorizer(stop_words='english')\n",
    "X_train_cvec= cvec.fit_transform(X_train)\n",
    "X_test_cvec = cvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9807441253263708"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "model = nb.fit(X_train_cvec, y_train)\n",
    "model.score(X_train_cvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913894324853229"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_cvec, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  TFIDF\n",
    "\n",
    "Now try a TFIDF model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tvec = TfidfVectorizer()\n",
    "X_train_tvec= tvec.fit_transform(X_train)\n",
    "X_test_tvec = tvec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8172323759791122"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "model = nb.fit(X_train_tvec, y_train)\n",
    "model.score(X_train_tvec, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8180039138943248"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_tvec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does that perform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming and Lemmatization\n",
    "Adding a Stemmer or Lemmatizer from nltk or spacy can be done several ways. We did something similar in global. I'm going to leave that for the solution, and we'll move to just using those in a pipeline because it's much more straightforward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD:05-Week/05-nlp/NLP_solution.ipynb
    "### This time let's make a function to lemmatize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(text):\n",
    "    '''return lemmatizeed list of words as a string from a document passed in '''\n",
    "    text = text.lower()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    return \"\".join([lemmer.lemmatize(word) for word in text])"
=======
    "##  Lemmatize"
>>>>>>> 10ab787919ea6967db9735bab54ef957e9363257:05-Week/05-nlp/NLP_starter_post_class_jeff.ipynb
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's make a function to lemmatize the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_lemmas(text):\n",
    "    '''return lowercased, lemmatizeed list of words as a string from a document passed in '''\n",
    "   \n",
    "    text = text.lower()\n",
    "    lemmer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmer.lemmatize(word) for word in text.split() ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's do this the smart way and make a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    CountVectorizer(preprocessor=split_into_lemmas),\n",
    "    MultinomialNB()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('countvectorizer',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1),\n",
       "                                 preprocessor=<function split_into_lemmas at 0x1a24b6f0e0>,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('multinomialnb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9158512720156555"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV\n",
    "\n",
    "Want to tune hyperparameters? Let's do it! 🚀\n",
    "\n",
    "### Warning, this might take 10min to run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(\n",
    "    countvectorizer__ngram_range=[(1, 1), (1, 2), (1, 3)],\n",
    "    multinomialnb__alpha=[.5, 1, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=pipe, param_grid=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('countvectorizer',\n",
       "                                        CountVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.int64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        preprocessor=<function split_into_lemmas at 0x1a24b6f0e0>,\n",
       "                                                        sto...\n",
       "                                                        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                        tokenizer=None,\n",
       "                                                        vocabulary=None)),\n",
       "                                       ('multinomialnb',\n",
       "                                        MultinomialNB(alpha=1.0,\n",
       "                                                      class_prior=None,\n",
       "                                                      fit_prior=True))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=None,\n",
       "             param_grid={'countvectorizer__ngram_range': [(1, 1), (1, 2),\n",
       "                                                          (1, 3)],\n",
       "                         'multinomialnb__alpha': [0.5, 1, 5]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=0)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.910958904109589"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'countvectorizer__ngram_range': (1, 1), 'multinomialnb__alpha': 0.5}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How about them apples? 🍎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add non-text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>type</th>\n",
       "      <th>user_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9yKzy9PApeiPPOUJEtnvkg</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>fWKvX83p0-ka4JS3dc6E5A</td>\n",
       "      <td>5</td>\n",
       "      <td>My wife took me here on my birthday for breakf...</td>\n",
       "      <td>review</td>\n",
       "      <td>rLtl8ZkDX5vH5nAx9C3q5Q</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZRJwVLyzEJq1VAihDhYiow</td>\n",
       "      <td>2011-07-27</td>\n",
       "      <td>IjZ33sJrzXqU-0X6U8NwyA</td>\n",
       "      <td>5</td>\n",
       "      <td>I have no idea why some people give bad review...</td>\n",
       "      <td>review</td>\n",
       "      <td>0a2KyEL0d3Yb1V6aivbIuQ</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6oRAC4uyJCsJl1X0WZpVSA</td>\n",
       "      <td>2012-06-14</td>\n",
       "      <td>IESLBzqUCLdSzSqm0eCSxQ</td>\n",
       "      <td>4</td>\n",
       "      <td>love the gyro plate. Rice is so good and I als...</td>\n",
       "      <td>review</td>\n",
       "      <td>0hT2KtfLiobPvh6cDC8JQg</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>_1QQZuf4zZOyFCvXc0o6Vg</td>\n",
       "      <td>2010-05-27</td>\n",
       "      <td>G-WvGaISbqqaMHlNnByodA</td>\n",
       "      <td>5</td>\n",
       "      <td>Rosie, Dakota, and I LOVE Chaparral Dog Park!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>uZetl9T0NcROGOyFfughhg</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6ozycU1RpktNG2-1BroVtw</td>\n",
       "      <td>2012-01-05</td>\n",
       "      <td>1uJFq2r5QfJG_6ExMRCaGw</td>\n",
       "      <td>5</td>\n",
       "      <td>General Manager Scott Petello is a good egg!!!...</td>\n",
       "      <td>review</td>\n",
       "      <td>vYmM4KTsC8ZfQBg-j5MWkw</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date               review_id  stars  \\\n",
       "0  9yKzy9PApeiPPOUJEtnvkg  2011-01-26  fWKvX83p0-ka4JS3dc6E5A      5   \n",
       "1  ZRJwVLyzEJq1VAihDhYiow  2011-07-27  IjZ33sJrzXqU-0X6U8NwyA      5   \n",
       "2  6oRAC4uyJCsJl1X0WZpVSA  2012-06-14  IESLBzqUCLdSzSqm0eCSxQ      4   \n",
       "3  _1QQZuf4zZOyFCvXc0o6Vg  2010-05-27  G-WvGaISbqqaMHlNnByodA      5   \n",
       "4  6ozycU1RpktNG2-1BroVtw  2012-01-05  1uJFq2r5QfJG_6ExMRCaGw      5   \n",
       "\n",
       "                                                text    type  \\\n",
       "0  My wife took me here on my birthday for breakf...  review   \n",
       "1  I have no idea why some people give bad review...  review   \n",
       "2  love the gyro plate. Rice is so good and I als...  review   \n",
       "3  Rosie, Dakota, and I LOVE Chaparral Dog Park!!...  review   \n",
       "4  General Manager Scott Petello is a good egg!!!...  review   \n",
       "\n",
       "                  user_id  cool  useful  funny  \n",
       "0  rLtl8ZkDX5vH5nAx9C3q5Q     2       5      0  \n",
       "1  0a2KyEL0d3Yb1V6aivbIuQ     0       0      0  \n",
       "2  0hT2KtfLiobPvh6cDC8JQg     0       1      0  \n",
       "3  uZetl9T0NcROGOyFfughhg     1       2      0  \n",
       "4  vYmM4KTsC8ZfQBg-j5MWkw     0       0      0  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yelp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = yelp_best_worst[['text', 'cool', 'useful']]\n",
    "y = yelp_best_worst['stars']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 4086 entries, 0 to 9999\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    4086 non-null   object\n",
      " 1   cool    4086 non-null   int64 \n",
      " 2   useful  4086 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 127.7+ KB\n"
     ]
    }
   ],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use `make_column_transformer` and scale our data\n",
    "\n",
    "##### fyi, MultinomialNB wants positive values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = make_column_transformer(\n",
    "    (CountVectorizer(preprocessor=split_into_lemmas, ngram_range = (1,1)), 'text'),\n",
    "    (MinMaxScaler(), ['cool', 'useful'])\n",
    ")\n",
    "\n",
    "# 'text' is not passed in a list - very confusing\n",
    "# # https://stackoverflow.com/a/56299794/4590385  \n",
    "\n",
    "# docs: https://github.com/scikit-learn-contrib/sklearn-pandas#map-the-columns-to-transformations\n",
    "# Be aware that some transformers expect a 1-dimensional input \n",
    "# (the label-oriented ones) while some others, like OneHotEncoder or Imputer, \n",
    "# expect 2-dimensional input, with the shape [n_samples, n_features]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('columntransformer',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('countvectorizer',\n",
       "                                                  CountVectorizer(analyzer='word',\n",
       "                                                                  binary=False,\n",
       "                                                                  decode_error='strict',\n",
       "                                                                  dtype=<class 'numpy.int64'>,\n",
       "                                                                  encoding='utf-8',\n",
       "                                                                  input='content',\n",
       "                                                                  lowercase=True,\n",
       "                                                                  max_df=1.0,\n",
       "                                                                  max_features=None,\n",
       "                                                                  min_...\n",
       "                                                                               1),\n",
       "                                                                  preprocessor=<function split_into_lemmas at 0x11d973950>,\n",
       "                                                                  stop_words=None,\n",
       "                                                                  strip_accents=None,\n",
       "                                                                  token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                                                  tokenizer=None,\n",
       "                                                                  vocabulary=None),\n",
       "                                                  'text'),\n",
       "                                                 ('minmaxscaler',\n",
       "                                                  MinMaxScaler(copy=True,\n",
       "                                                               feature_range=(0,\n",
       "                                                                              1)),\n",
       "                                                  ['cool', 'useful'])],\n",
       "                                   verbose=False)),\n",
       "                ('multinomialnb',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe = make_pipeline(\n",
    "    ct,\n",
    "    MultinomialNB()\n",
    ")\n",
    "\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9070450097847358"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     1944\n",
       "1      601\n",
       "2      227\n",
       "3      118\n",
       "4       53\n",
       "5       38\n",
       "6       19\n",
       "8       12\n",
       "7       12\n",
       "10       7\n",
       "14       6\n",
       "9        5\n",
       "17       4\n",
       "11       4\n",
       "16       3\n",
       "12       3\n",
       "13       2\n",
       "23       1\n",
       "32       1\n",
       "15       1\n",
       "19       1\n",
       "27       1\n",
       "77       1\n",
       "Name: cool, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['cool'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Did adding the two non-text columns help?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus ⭐️\n",
    "\n",
    "- Try differentiating the 4 star vs. 5 star reviews\n",
    "- Try a logistic regression or knn model\n",
    "- Try more transformer hyperparameters\n",
    "- Use GridSearch with the final pipeline -  hint: `columntransformer__countvectorizer__ngram_range` is how to create grid elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "You've seen how to use `CountVectorizer` and `TfidfVectorizer` for NLP with a pipeline and grid searching.\n",
    "\n",
    "### Check for Understanding\n",
    "\n",
    "- What is bag of words?\n",
    "- What is TF-IDF?\n",
    "\n",
    "- What are stop words?\n",
    "- What are n-grams?\n",
    "\n",
    "- What's a document?\n",
    "- What's a corpus?\n",
    "\n",
    "- How are stemming and lemmatization different?\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### NLP is a big area, but you've covered lots of the core aspects! 🎉\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
