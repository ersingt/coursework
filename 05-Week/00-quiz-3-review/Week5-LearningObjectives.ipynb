{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5 Learning Objectives\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing\n",
    "\n",
    "### Define and implement tokenizing, lemmatizing, and stemming.\n",
    "**Tokenizing**\n",
    "- Tokenizing means splitting your text into minimal meaningful units.\n",
    "\n",
    "**Lemmatizing**\n",
    "- Converting a word to its based dictionary from from its conjugated or otherwise unnatural state it is in.\n",
    "    - \"Running\" -> \"Run\"\n",
    "    - \"Agility\" -> \"Agile\"\n",
    "    - \"Continuous\" -> \"Continue\"\n",
    "    \n",
    "**Stemming**\n",
    "- Chopping of the end of a word in hopes of whittling it down to the base.  This is much more crude and, because of the English language, not as great as _Lemmatization_.  It searches for common endings/suffixes on the end of words like \"ing\", \"ed\", \"ous\" and removes them in _hopes_ of cutting the word down to its original value. \n",
    "\n",
    "\n",
    "\n",
    "### Describe what RegEx does.\n",
    "- _Regex is a useful tool to select patterns in text and help us clean/preprocess text._\n",
    "- Regex == Regular Expressions\n",
    "- Regex allows of to create specific and general patterns to parse through text data to clean it or separate it.\n",
    "\n",
    "### Apply sentiment analysis.\n",
    "**Sentiment Analysis**\n",
    "- Simply put, Sentiment Analysis is the process of breakdown and analyzing text data to find the key concept of a sentence, paragraph or tweet and, more importantly, the overall positive or negative sentiment being used in the document. \n",
    "\n",
    "### Preprocess text data.\n",
    "Typically, Text is converted from sentences via a Word-2-Vector concept, where each row represents a document (sentence, paragraph, tweet) and the columns represent words.  Having a value != 0 in a column for a row, indicates that word appears in said document, having a 0 indicates that a word _does not_ appear.  \n",
    "There are Two common means of converting preprocessing the text to make it ready for modeling.\n",
    "\n",
    "**Count Vectorizer**\n",
    "- The Basic vectorizer, values will be represented as counts of how often that word appears in the document. \n",
    "\n",
    "**TFIDF Vectorizer**\n",
    "- Term Frequency Inverse Document Frequency.  This takes into consideration how often a word appears in a document and how often it appears in the corpus (all documents/rows) \n",
    "\n",
    "$TFIDF = (Document Frequency)*(Log(\\frac{Number Of Documents In Corpus}{Number Of Documents Term Appears In})$\n",
    "\n",
    "[Tifi-Difi is about what matters](https://planspace.org/20150524-tfidf_is_about_what_matters/)\n",
    "\n",
    "### Interpret word vectors.\n",
    "\n",
    "- Converting a document into a single row in a dataframe. \n",
    "- Rows represent the document by counting occurances of words or lack of words.  \n",
    "\n",
    "### Stop Words\n",
    "\n",
    "- Some words are commonly used and provide no legitimate information about the content of the text.\n",
    "- Does the number of times the words `is`, `are`, `be` appear in a text really provide meaningful information aboout the text? \n",
    "- Words like these are called stop words. The actual collection of stop words varies from library to library -- the scikit-learn stop words may be different from the NLTK, spaCy, or gensim stop words but they serve the same purpose -- to tell the vectorizer (whatever kind) which words to ignore.\n",
    "\n",
    "### N-Gram Range\n",
    "\n",
    "CountVectorizer has the ability to capture $n$-word phrases, also called $n$-grams. Consider the following:\n",
    "\n",
    "> The quick brown fox jumped over the lazy dog.\n",
    "\n",
    "In the example sentence, the 2-grams (aka bi-grams) are:\n",
    "- 'the quick'\n",
    "- 'quick brown'\n",
    "- 'brown fox'\n",
    "- 'fox jumped'\n",
    "- 'jumped over'\n",
    "- 'over the'\n",
    "- 'the lazy'\n",
    "- 'lazy dog'\n",
    "\n",
    "The ngram_range determines what $n$-grams should be considered as features.\n",
    "\n",
    "`cvec = CountVectorizer(ngram_range=(1,2))` # Captures every single word and every 2-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Webscraping \n",
    "### The requests Library\n",
    "The requests library is a library for submitting HTTP requests from Python. Despite its frequent use, it's not included in the Python standard library. You'll need to pip install requests yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "url_squirtle = \"https://pokeapi.co/api/v2/pokemon/squirtle\"\n",
    "req = requests.get(url_squirtle)\n",
    "req.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Status Codes\n",
    "- 1XX - Informational\n",
    "- 2XX - Successful\n",
    "- 3XX - Redirection\n",
    "- 4XX - Client Error\n",
    "- 5XX - Server Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*Note:* we didn't go over Naive Bayes , so **Naive Bayes will not be in the quiz.** Having said that, it's a useful information/material to know so I've added it to the review. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Enrichment : Naive Bayes (NOT IN THE QUIZ - here for your reference!)\n",
    " \n",
    "### Intuitively explain how Bayes' Theorem can be used as a modeling tactic. \n",
    "Bayes Theorm focuses on the Probability of observing an event give a previously observed event or series or previously observed events. \n",
    "   - Target : Probability of Observing the event\n",
    "   - Features : The Observed event(s)\n",
    "\n",
    "Probability of Observing A given already observing B.\n",
    "\n",
    "### $P(A|B) = \\frac{P(B|A)P(A)}{P(B)}$\n",
    "\n",
    "Probability of observing Spam given the list of Words ($W_1$ through $W_N$)\n",
    "\n",
    "### $P(Spam|Words) = \\frac{P(W_1|Spam) P(W_2|Spam) ... P(W_n|Spam) P(Spam)}{P(W_1)P(W_2)...P(W_n)}$\n",
    "\n",
    "\n",
    "\n",
    "### Implement Naive Bayes in scikit-learn.\n",
    "\n",
    "Implementation is same as all other ML Mmodels.\n",
    "- [SKLearn Naive Bayes Overview](http://scikit-learn.org/stable/modules/naive_bayes.html)\n",
    "- [SKLearn Bernoulli NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html)\n",
    "    - When our columns in our feature matrix are 1s and 0s.\n",
    "- [SKLearn Multinomial NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html)\n",
    "    - When our columns in our feature matrix are integer counts\n",
    "- [SKLearn Gaussian NB](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html)\n",
    "    - When our columns in our feature matrix are normally distributed.\n",
    "    \n",
    "_In the event that our columns are a mixture of desired columns, choose the model with the broader scope. (Bottom Up)_     \n",
    "\n",
    "### Discuss assumptions, advantages, and disadvantages of Naive Bayes as a classifier.\n",
    "\n",
    "\n",
    "**Assumptions**\n",
    "- \"All features are independent of one another\"\n",
    "\n",
    "**Advantages**\n",
    "- Relatively simple to calculate probabilities.\n",
    "- Empirically, predictions are surprisingly accurate.\n",
    "\n",
    "**Disadvantages**\n",
    "- Given the assumption about feature independence, its incredibly unrealistic, especially with text data.\n",
    "- While Classifications tend to be accurate, predicted probabilities tend to be bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Again, Naive Bayes Classifier won't be in Quiz 3 (but you do need to use one in Project 3, so it is useful information!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
