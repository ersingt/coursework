{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMRHD38LnM6T"
   },
   "source": [
    "# Week 4 Learning Objectives\n",
    "\n",
    "-----\n",
    "\n",
    "## Classification & Logistic Regression\n",
    "\n",
    "### Distinguish between regression and classification problems.\n",
    "Supervised machine learning and data science problems fall into two broad categories: regression and classification. Both problemswant to make a prediction about observations that haven't been seen based on information from observations that have been seen.  \n",
    "**Regression** problems seek to predict a value, like the number of minutes until a bus arrives, the number of fishforks set on a table, or the revenue of a division for the next quarter.  \n",
    "**Classification** problems, on the other hand, try to predict membership in a group, where groups are mutually exclusive. In the case of \"Mean Girls,\" we might look to predict \"Plastic\" or \"Non-Plastic.\" Any data point we have has to fall into one and only one of these classes.\n",
    "\n",
    "### Understand how Logistic Regression is similar & different from linear Regression.\n",
    "\n",
    "**Similarities**\n",
    "- Both a Parametric Regressors\n",
    "- Both utilize a linear equation to arrive at a prediction.\n",
    "\n",
    "**Differences**\n",
    "- Linear Reg. is used to predict a continuous value.\n",
    "- Logistic Reg. is used to predict a probability of a class.  \n",
    "- Logistic Regression typically needs iterative learning and use of gradient descent and regularization to work well.\n",
    "\n",
    "\n",
    "### Understand the math behind the Logit Function\n",
    "- The Log function is used to find the exponent value needed to take a base value to the target value.\n",
    "- $ log_{10}(100) = 2 $ Basically states that in order to take the base $10$ to the value of $100$ you need to apply a power of $2$ to the base.\n",
    "- The Logit Equation applies this same log aspect to convert and odds ratio to a log odds value. $log(P) = log(\\frac{p}{1-p})$\n",
    "\n",
    "\n",
    "\n",
    "### Code and Calculate the Odds and Log Odds ratios.\n",
    "- **Odds** = $\\frac{p}{1-p}$\n",
    "- **Log Odds** = $log(\\frac{p}{1-p})$\n",
    "_Where $p$ is the probability of some observed event occuring_\n",
    "\n",
    "\n",
    "### Understand how to interpret the coefficients of a Logistic Regression\n",
    "- First off, the Logistic Function we're trying to solve is $\\frac{1}{1+e^{-\\beta_0 + \\sum\\limits_{i=1}^j \\beta_i X_i}}$\n",
    "- There is a linear equation that gets applied the Euler's number.  This equation is solved and optimized via the Linear Equation, Gradient Descent and Regularization with a target of the log odds ratio.\n",
    "- The Result of this is a $P(Y_i=1|x_i)$\n",
    "- Because there is a linear equation in the Logistic Formula, we need to be able to interpret the coefficients.\n",
    "- Unlike Lin-Reg, coefficients need to be transformed in order to be interpreted.  $e^{\\beta}$ will provide a value that can be interpreted as \" A 1 unit increase in $\\beta\"$ is $e^{\\beta}$ as likely to be $y = 1$.\n",
    "- IF $e^{\\beta} > 1 $ than you can drop the $1$ and move the decimal over two places to the right to create a percentage that can be interpreted as \" A 1 unit increase in $\\beta$ makes Y = 1 _percent_ more likely.\"\n",
    "\n",
    "### Know the Benefits of the Logistic Regression as a Classifier\n",
    "- It's a classification algorithm that shares similar properties to linear regression.\n",
    "- It's efficient and is a very common classification algorithm.\n",
    "- The coefficients in a logistic regression model are interpretable (albeit somewhat complex); they represent the change in log-odds caused by the input variables.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cor_lxxWnM6x"
   },
   "source": [
    "------\n",
    "\n",
    "## KNN and Classification Metrics\n",
    "\n",
    "### Intuition behind the KNN algorithm\n",
    "- KNN is a simple ML model that takes into consideration know values and makes a prediction based on the \"K\" most similar (closest) known observations.\n",
    "- KNN is a Parametric Distance based algorithm with 1 very important Hyperparamter, \"K\", and several others of note.\n",
    "    - `n_neighbors` : This is the \"K\" argument in SKLearn, which denotes the number of known observations to consider when making a prediction.\n",
    "    - `weights`     : Allows you to adjust how much each neighbor contributes to the prediction.\n",
    "    - `metric`      : Allows you to change the distance based algorithm that is used to find the closest points. I.E. Minkowski(Euclidean) or Manhattan.  \n",
    "    [Available Distance Metrics](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)\n",
    "\n",
    "\n",
    "### Implementing KNN with sklearn\n",
    "- `from sklearn.neighbors import KNeighborsClassifier`\n",
    "- There is a complementary regression model called `KNeighborsRegressor` which uses the same principal of identifying the most similar observations and taking an average of their target to create a prediction.\n",
    "\n",
    "\n",
    "### Define true positives, true negatives, false positives, false negatives.  \n",
    "When working with classification problems, its important to state a reference class by which you will be associating Positives and Negatives with.  With Binary this is relatively easy because there is typically a distinguishment between \"True & False\" or \"Happen & Doesn't Happen\" or \"Is & Isn't\".\n",
    "- True Positives : These are observations in the reference class that your model _correctly_ predicted as being in said reference class.\n",
    "    - Predicted Cancerous : Is Cancerous\n",
    "- True Negatives : These are observations NOT in the reference class that your model _correctly_ predicted as NOT being in said reference class.\n",
    "    - Predicted Benign  : Is Benign\n",
    "- False Positives (Type 1): These are observations NOT in the reference class that your model _incorrectly_ predicted as being in said reference class.\n",
    "    - Predicted Cancerous  : Is Benign\n",
    "- False Negatives (Type 2): These are observations in the reference class that your model _incorrectly_ predicted as NOT being in said reference class.\n",
    "    - Predicted Benign  : Is Cancerous\n",
    "\n",
    "\n",
    "### Construct a confusion matrix.\n",
    "- [SKLearn Confusion Matrix Documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n",
    "\n",
    "|n = 175  | Predicted True | Predicted False |\n",
    "|:---|:---:|:---:|\n",
    "|**Is True**|91 | 13 |\n",
    "|**Is False**| 29 | 42 |\n",
    "\n",
    "\n",
    "### Calculate accuracy, misclassification rate, sensitivity, specificity, precision, and Brier score.\n",
    "\n",
    "_We'll use the above confusion matrix for these examples_\n",
    "\n",
    "\n",
    "- **Accuracy** : Proportion of _correctly_ labeled predictions.\n",
    "\n",
    "    #### $\\frac{TP+TN}{n} = \\frac{91+42}{175} = 0.76$\n",
    "\n",
    "\n",
    "- **Missclassification Rate** : Proportion of _incorrectly_ labeled predictions. (1 - accuracy)\n",
    "\n",
    "    #### $\\frac{FP+FN}{n} = \\frac{29+13}{175} = 0.24$\n",
    "\n",
    "\n",
    "- **Sensitivity** : Of the actual positive observations, how many did we correctly identify? This is also known as \"Recall\" or \"True Positive Rate\"\n",
    "\n",
    "    #### $\\frac{TP}{TP+FN} = \\frac{91}{91+13} = 0.875$\n",
    "\n",
    "\n",
    "- **Specificity** : Of the \"Negative\" observations, how many did we correctly prediction?\n",
    "\n",
    "    #### $\\frac{TN}{TN+FP} = \\frac{42}{42+29} = 0.591$\n",
    "\n",
    "\n",
    "- **Precision** : How many of our \"Positive\" predictions are correct?\n",
    "\n",
    "    #### $\\frac{TP}{TP+FP} = \\frac{91}{91+29} = 0.758$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Sensitivity & Specificity  |  Precision & Recall\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://raw.githubusercontent.com/dstrodtman/dsi_public/master/images/Sen-Spe.png)  |  ![](https://raw.githubusercontent.com/dstrodtman/dsi_public/master/images/Pre-Rec.png)\n",
    "\n",
    "\n",
    "\n",
    "### Compare the \"cost\" or \"badness\" of false positives versus false negatives.\n",
    "- Using/visualizing the ROC is a good way to approach the \"Cost\" associated with _your specific problem_. In almost all situations, your model will never be perfect and you will have some misclassifications.  Depending on your problem, you will need to choose whether your will need to optimize to reduce False Positives or False Negatives.  \n",
    "  - Optimizing to reduce False Negatives.  Say you are trying to predict if a tumor os cancerous of benign.  You will want to decrease False negatives at all cost, as these are observations your model predicts as being \"Non-Cancerous\", but turn out to be \"Cancerous\".  On the other hand predicting \"Cancerous\" when a tumor is \"benign\" is not as detrimental to the individual who is being assessed as they can take precautions and get a second opinion.\n",
    "\n",
    "  - Optimizing to Reduce False Positives. Say you are a loan provider and are trying to predict whether or not an applicant will default on their loan.  You will probably want to optimize your model so you never give out a loan to someone that will default on it.  In this scenario a False Positive is an individual you would provide a load who would default, and a False Negative is someone not given a loan that would not default.   In this imaginary company having a loan defaulted on is more detrimental to the business than missing a load opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC AUC\n",
    "\n",
    "\n",
    "TPR vs FPR\n",
    "\n",
    "### Describe the inverse relationship between sensitivity and specificity.\n",
    "- Sensitivity : Of all the Class 1 observations, how many did we correctly identify?\n",
    "- Specificity : Of all the Non-Class 1 observations, how many did we correctly Identify?\n",
    "\n",
    "Typically, there is a general trade off and a gray area of observations that are difficult to classify as its a pretty even mixture of the two or more classes and thus nearly impossible to fully separate.  Maximizing sensitivity and correctly classifying all of the Class 1 observations typically results in us having several false positives. Increasing FPs increases the denominator of  specificity and thus will reduce it.  On the other hand Minimizing False Positives will maximize the specificity at the cost of increasing False Negatives thus decreasing the sensitivity score.\n",
    "\n",
    "### Construct the ROC Curve.\n",
    "- The Receiver Operators Curve is used to show us the relationship between the True Positives and False positives given a predict probability threshold(typically not visualized).\n",
    "- Typically we construct this curve to analyze optimal threshold or how well the model generalizes. \n",
    "- Points closest to the top left corner are ideal.  a straight diagonal line from bottom left to top right essentially means your model is worthless.\n",
    "- You will need the `predict_proba()` from a binary classifier(output) to construct a ROC. \n",
    "\n",
    "### Understand how AUC ROC is calculated and interpret AUC ROC.\n",
    "- Area Under the Curve. If your AUC = 1 your model is perfect, if AUC = 0.5 its worthless.  It's an area under a curve.\n",
    "\n",
    "![](../assets/roc-auc.png)\n",
    "\n",
    "\n",
    "### Define unbalanced classes and identify scenarios with unbalanced classes.\n",
    "- Unbalanced class are class distributions in which 1 or more class(es) significantly out numbers the other(s). \n",
    "- Unbalanced classes make accuracy score almost a worthless metric.\n",
    "- In the event of a sample with 1000 observations, and 990 of Class A and 10 are Class B, predicting Class A every time, our baseline, will result in a accuracy of 99%.  \n",
    "- When one class has significantly fewer observations than others, it makes it difficult for the model to pick up what makes that class stand out and the features predictive of that class.  \n",
    "\n",
    "\n",
    "### Describe methods for handling unbalanced classes.\n",
    "\n",
    "- Randomly reducing the number of observations of Class A\n",
    "- Doubling or tripling observations of Class B\n",
    "- Boostraping\n",
    "- Stratifing samples\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "### Define 'loss function'\n",
    "A loss function or objective function or cost function is the metric that our algorithm optimizes on. It tells us how well (or how badly) our model is performing. In the case of an ordinary least squares regression, the loss function is the sum of squared errors (SSE).\n",
    "\n",
    "### Define 'regularization'\n",
    "Regularization is the group of methods for constraining (regularizing) the coefficients of a linear model in order to reduce the error due to variance.\n",
    "\n",
    "### Least Squares Loss Function\n",
    "\n",
    "---\n",
    "\n",
    "Ordinary least squares regression minimizes the mean squared error (MSE) to fit the data:\n",
    "\n",
    "### $$ \\text{minimize:}\\; SSE(\\beta_0, \\beta_1, ...) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "Where our model predictions for $y$ are based on the sum of the $\\beta_0$ intercept and the products of $\\beta_i$ with $x_i$.\n",
    "\n",
    "### Defining the Lasso\n",
    "\n",
    "---\n",
    "\n",
    "Now we do the same thing as above but for the Lasso. You will be able to see how the coefficients change differently for both!  But first, let's define lasso.\n",
    "\n",
    "Lasso regression takes a different approach. Instead of adding the sum of squared $\\beta$ coefficients to the RSS, it adds the sum of the absolute values of the $\\beta$ coefficients:\n",
    "\n",
    "### $$ \\text{minimize:}\\; SSE + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$|\\beta_j|$ is the absolute value of the $\\beta$ coefficient for variable $x_j$.\n",
    "\n",
    "$\\alpha$ is the strength of the regularization penalty component in the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### Defining the Ridge\n",
    "\n",
    "---\n",
    "\n",
    "### $$ \\text{minimize:}\\; SSE+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$\\beta_j^2$ is the squared coefficient for variable $x_j$.\n",
    "\n",
    "$\\sum_{j=1}^n \\beta_j^2$ is the sum of these squared coefficients for every variable in the model. This does **not** include the intercept $\\beta_0$.\n",
    "\n",
    "$\\alpha$ is a constant for the _strength_ of the regularization parameter. The higher the value, the greater the impact of this new component in the loss function. If the value was zero, we would revert back to just the least squares loss function. If the value was a billion, however, the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term.\n",
    "\n",
    "---\n",
    "\n",
    "### Defining the Elastic Net\n",
    "\n",
    "---\n",
    "\n",
    "The Elastic Net combines the Ridge and Lasso penalties.  It adds *both* penalties to the loss function:\n",
    "\n",
    "> \"[Elastic Net] allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\"\n",
    "-- sklearn docs\n",
    "\n",
    "#### $$ \\text{minimize:}\\; SSE + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\rho\\sum_{j=1}^p |\\beta_j| + \\alpha(1-\\rho)\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "In the elastic net, the effect of the ridge versus the lasso is balanced by the $\\rho$ parameter.  It is the ratio of Lasso penalty to Ridge penalty and must be between zero and one.\n",
    "\n",
    "- **Ridge** is good at \"shrinking\" model coefficients.\n",
    "- **Lasso** is good at eliminating coefficients.\n",
    "- **ElasticNet** combines Ridge and Lasso.\n",
    "\n",
    "### Bottom line?\n",
    "\n",
    "In all cases, \"regularization strength\" is defined by a parameter $\\alpha$ (sometimes called $\\lambda$):\n",
    "- Increase $\\alpha$ (turn up regularization) \n",
    "    - Increase bias\n",
    "    - Decrease variance\n",
    "- Decrease $\\alpha$ (turn down regularization) \n",
    "    - Decrease bias\n",
    "    - Increase variance \n",
    "    \n",
    "---\n",
    "\n",
    "- The Ridge is best suited to deal with multicollinearity. \n",
    "- Lasso also deals with multicollinearity between variables, but in a more brutal way (it \"zeroes out\" the less effective variable).\n",
    "- The Lasso is particularly useful when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you by forcing coefficients to be zero.\n",
    "- Elastic Net combines both.\n",
    "\n",
    "### Regularization and the Bias-Variance Tradeoff\n",
    "We can think of regularization as exchanging error due to variance for error due to bias. Regularization does this by reducing or eliminating the coefficients of the independent variables, which reduces the complexity of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5hSDUwCnM7G"
   },
   "source": [
    "##  Gridsearching, Hyperparameters and Pipelines\n",
    "\n",
    "### Describe what the terms gridsearch and hyperparameter mean.\n",
    "- Gridsearch is the act of searching a potentially multi-dimensional grid of possible values for the optimal combination.\n",
    "- Hyperparameters are values that can be adjusted in a model to have effects on how the model performs.\n",
    "\n",
    "### Build a gridsearching procedure from scratch.\n",
    "```python\n",
    "\n",
    "# for param1 in param1_list:\n",
    "#     for param2 in param2_list:\n",
    "#         for param3 in param3_list:\n",
    "#             Instantiate model with current params\n",
    "#             fit the model\n",
    "#             score the model\n",
    "#             store the scores and params list for each model\n",
    "           \n",
    "# search through Model_Score_Dict for max score \n",
    "# return Max score and ist associated params.\n",
    "```        \n",
    "\n",
    "### Apply sklearn's GridSearchCV object with basketball data to optimize a KNN model.\n",
    "\n",
    "\n",
    "### Use and evaluate attributes of the gridsearch object.\n",
    "\n",
    "\n",
    "### Describe the pitfalls of searching large hyperparameter spaces.\n",
    "- More hyper parameters = more combinations = more models = more time/computational expense\n",
    "- If you have a lot of data, model building takes time as well.\n",
    "- If you have a complex or deep model each model times more time to build as well.\n",
    "- Multiply this by however many Hyperparameters you're searching over.\n",
    "- You will want to use model based EDA to identify small hyperparameter spaces to search through instead of brute forcing and searching through ever value in existence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqecqjR0yZ0L"
   },
   "source": [
    "### Pipeline Syntax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6CO7Nchyh5H"
   },
   "source": [
    "We set up a pipeline by passing a list of tuples in the format\n",
    "```\n",
    "('string_name', ClassObject())\n",
    "```\n",
    "Note that we can name our steps beforehand (each of the methods that we're using are a class in sklearn).\n",
    "```\n",
    "lasso = Lasso()\n",
    "('lasso', lasso)\n",
    "```\n",
    "\n",
    "We can include as many steps as we'd like. Look at the following example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqtaQaLVzS6k"
   },
   "source": [
    "After we create/set up the pipeline, we `fit` it on our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmwvf__Mz1iN"
   },
   "source": [
    "Then we can `score` on our training set and testing set, generate predictions using `predict`, etc. - anything you would do with a \"regular\" model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpLqnj8QydqS"
   },
   "source": [
    "### GridSearch Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SswTjsaI0CHa"
   },
   "source": [
    "`GridSearch` accepts a `Pipeline` object as an estimator and a param grid.\n",
    "\n",
    "The param grid uses the `string_name`s from your pipeline followed by a dunder `__` and the argument name for that particular step. You then provide an iterable to search over (generally a list or a range-style object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:  \n",
    "parameters = {'hyper_parameter__name of hyperparameter' : [different values for hyperparameter]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1pPWNNe1GwC"
   },
   "source": [
    "You can also specify the number of folds using `cv`. Default is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUZCdrmt1OXj"
   },
   "source": [
    "We use this the same as other models, `fit`ting and `score`ing like normal (but now using the hyperparameters that gave us the best results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzHKqHd41hdE"
   },
   "source": [
    "Note that we'll use our `best_estimator_` to access the `Pipeline` that was fit with our `best_params_`.\n",
    "\n",
    "Within the `best_estimator_`there is a dictionary called `named_steps`. We can use our `string_names` to access the steps in our `Pipeline`. This is where we'll go to access info about the transformations and parameters done at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use and evaluate attributes of the gridsearch object.\n",
    "- A fit GridsearchCV object has the same methods as our normal fit models, and can be used with .predict and .score generate cross-validated scores and predictions on test data.  \n",
    "- The GridsearchCV object also has attributes that return the .best_params_, the .best_estimator_, and .best_score_ of the estimator chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```params = {\n",
    "      parmeter_1:[3, 5, 7]  \n",
    "      parmeter_2:[0.01, 0.1, 1.0]  \n",
    "      paramter_3:('x', 'y', 'z')\n",
    "}  ```\n",
    "We 3 values in each parameter, so it's $3x3x3=27$.  \n",
    "If we're using CV, than we multiply it by the number of folds (so CV=3 would be $3x3x3x3$) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week4-LearningObjectives.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
