{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMRHD38LnM6T"
   },
   "source": [
    "# Week 4 Learning Objectives\n",
    "\n",
    "-----\n",
    "\n",
    "## Classification & Logistic Regression\n",
    "\n",
    "### Distinguish between regression and classification problems.\n",
    "Supervised machine learning problems fall into two broad categories: regression and classification. In both cases we want to make predictions about observations that haven't been seen based on information from observations that have been seen.  \n",
    "\n",
    "**Regression** problems seek to predict a value, such as the number of minutes until a bus arrives, the number of fishforks set on a table, or the revenue of a division for the next quarter.  \n",
    "\n",
    "**Classification** problems, on the other hand, try to predict membership in a group, where groups are mutually exclusive. In the case of \"Mean Girls,\" we might look to predict \"Plastic\" or \"Non-Plastic.\" Any data point we have has to fall into one and only one of these classes.\n",
    "\n",
    "### Understand how Logistic Regression is similar & different from linear Regression.\n",
    "\n",
    "**Similarities**\n",
    "- Both a Parametric Regressors\n",
    "- Both use a linear equation to arrive at a prediction.\n",
    "\n",
    "**Differences**\n",
    "- Linear Regression is used to predict a continuous value.\n",
    "- Logistic Regression is used to predict a probability of a class.  \n",
    "- Logistic Regression typically needs iterative learning and use of gradient descent and regularization to work well.\n",
    "\n",
    "\n",
    "### Understand the math behind the Logit Function\n",
    "- The Log function is used to find the exponent value needed to take a base value to the target value.\n",
    "- $ log_{10}(100) = 2 $ Basically states that to take the base $10$ to the value of $100$ you need to apply a power of $2$ to the base.\n",
    "- The Logit Equation applies this same log aspect to convert an odds ratio to a log odds value. $log(P) = log(\\frac{p}{1-p})$\n",
    "\n",
    "\n",
    "\n",
    "### Code and Calculate the Odds and Log Odds ratios.\n",
    "- **Odds** = $\\frac{p}{1-p}$\n",
    "- **Log Odds** = $log(\\frac{p}{1-p})$\n",
    "_Where $p$ is the probability of some observed event occuring_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nMRHD38LnM6T"
   },
   "source": [
    "### Understand how to interpret the coefficients of a Logistic Regression\n",
    "- First off, the Logistic Function we're trying to solve is $\\frac{1}{1+e^{-\\beta_0 + \\sum\\limits_{i=1}^j \\beta_i X_i}}$\n",
    "- There is a linear equation that gets applied the Euler's number.  This equation is solved and optimized via the Linear Equation, Gradient Descent and Regularization with a target of the log odds ratio.\n",
    "- Because there is a linear equation in the Logistic Formula, we can interpret the coefficients.\n",
    "- In sklearn, after fitting a LogisticRegression model named logreg, you can get the coefficient with `logreg.coef_`\n",
    "- If the coefficient of a feature is 3.2\n",
    "**Interpretation:** A 1-unit increase in the feature is associated with a 3.2-unit increase in the log odds of the outcome variable, all other things held constant.\n",
    "\n",
    "\n",
    "### Know the Benefits of the Logistic Regression as a Classifier\n",
    "- It's a classification algorithm that shares similar properties to linear regression.\n",
    "- It's efficient and is a very common classification algorithm.\n",
    "- The coefficients in a logistic regression model are interpretable (albeit somewhat complex); they represent the change in log-odds caused by the input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cor_lxxWnM6x"
   },
   "source": [
    "------\n",
    "\n",
    "## KNN and Classification Metrics\n",
    "\n",
    "### Intuition behind the KNN algorithm\n",
    "- KNN is a simple ML model that takes into consideration know values and makes a prediction based on the \"K\" most similar (closest) known observations.\n",
    "- KNN is a Parametric Distance based algorithm with 1 very important Hyperparamter, \"K\", and several others of note.\n",
    "    - `n_neighbors` : This is the \"K\" argument in SKLearn, which denotes the number of known observations to consider when making a prediction.\n",
    "    - `weights`     : Allows you to adjust how much each neighbor contributes to the prediction.\n",
    "    - `metric`      : Allows you to change the distance based algorithm that is used to find the closest points. I.E. Minkowski(Euclidean with the default value of p) or Manhattan.  \n",
    "    [Available Distance Metrics](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html)\n",
    "\n",
    "\n",
    "### Implementing KNN with sklearn\n",
    "- `from sklearn.neighbors import KNeighborsClassifier`\n",
    "- There is a complementary regression model called `KNeighborsRegressor` which uses the same principal of identifying the most similar observations and taking an average of their target to create a prediction.\n",
    "\n",
    "\n",
    "### Define true positives, true negatives, false positives, false negatives.  \n",
    "When working with classification problems, its important to state a reference class by which you will be associating Positives and Negatives with.  With Binary this is between \"True & False\" or \"Happen & Doesn't Happen\" or \"Is & Isn't\".\n",
    "- True Positives : These are observations in the reference class that your model _correctly_ predicted as being in said reference class.\n",
    "    - Predicted Cancerous : Is Cancerous\n",
    "- True Negatives : These are observations NOT in the reference class that your model _correctly_ predicted as NOT being in said reference class.\n",
    "    - Predicted Benign  : Is Benign\n",
    "- False Positives (Type 1): These are observations NOT in the reference class that your model _incorrectly_ predicted as being in said reference class.\n",
    "    - Predicted Cancerous  : Is Benign\n",
    "- False Negatives (Type 2): These are observations in the reference class that your model _incorrectly_ predicted as NOT being in said reference class.\n",
    "    - Predicted Benign  : Is Cancerous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cor_lxxWnM6x"
   },
   "source": [
    "### Construct a confusion matrix.\n",
    "- [SKLearn Confusion Matrix Documentation](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)\n",
    "\n",
    "|n = 175  | Predicted True | Predicted False |\n",
    "|:---|:---:|:---:|\n",
    "|**Is True**|91 | 13 |\n",
    "|**Is False**| 29 | 42 |\n",
    "\n",
    "\n",
    "### Calculate accuracy, misclassification rate, sensitivity, specificity, precision.\n",
    "\n",
    "_We'll use the above confusion matrix for these examples_\n",
    "\n",
    "\n",
    "- **Accuracy** : Proportion of _correctly_ labeled predictions.\n",
    "\n",
    "    #### $\\frac{TP+TN}{n} = \\frac{91+42}{175} = 0.76$\n",
    "\n",
    "\n",
    "- **Missclassification Rate** : Proportion of _incorrectly_ labeled predictions. (1 - accuracy)\n",
    "\n",
    "    #### $\\frac{FP+FN}{n} = \\frac{29+13}{175} = 0.24$\n",
    "\n",
    "\n",
    "- **Sensitivity** : Of the actual positive observations, how many did we correctly identify? This is also known as \"Recall\" or \"True Positive Rate\"\n",
    "\n",
    "    #### $\\frac{TP}{TP+FN} = \\frac{91}{91+13} = 0.875$\n",
    "\n",
    "\n",
    "- **Specificity** : Of the actual negative observations, how many did we correctly predict?\n",
    "\n",
    "    #### $\\frac{TN}{TN+FP} = \\frac{42}{42+29} = 0.591$\n",
    "\n",
    "\n",
    "- **Precision** : How many of our \"Positive\" predictions are correct?\n",
    "\n",
    "    #### $\\frac{TP}{TP+FP} = \\frac{91}{91+29} = 0.758$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cor_lxxWnM6x"
   },
   "source": [
    "Sensitivity & Specificity  |  Precision & Recall\n",
    ":-------------------------:|:-------------------------:\n",
    "![](https://raw.githubusercontent.com/dstrodtman/dsi_public/master/images/Sen-Spe.png)  |  ![](https://raw.githubusercontent.com/dstrodtman/dsi_public/master/images/Pre-Rec.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Cor_lxxWnM6x"
   },
   "source": [
    "### Compare the \"cost\" or \"badness\" of false positives versus false negatives.\n",
    "- Using/visualizing the ROC is a good way to approach the \"Cost\" associated with _your specific problem_. In almost all situations, your model will never be perfect and you will have some misclassifications.  Depending on your problem, you need to choose whether to optimize to reduce False Positives or False Negatives.  \n",
    "\n",
    "#### Optimizing to reduce False Negatives.  \n",
    "  \n",
    "- Say you are trying to predict wheter a tumor is cancerous or benign.  You will want to decrease False negatives. These are observations your model predicts as being \"Non-Cancerous\", but turn out to be \"Cancerous\".  \n",
    "  \n",
    "- On the other hand predicting \"Cancerous\" when a tumor is \"benign\" is not as detrimental to the individual who is being assessed.\n",
    "\n",
    "\n",
    "#### Optimizing to Reduce False Positives. \n",
    "  \n",
    "- Say you are a loan provider and are trying to predict whether or not an applicant will default on their loan.  You will probably want to optimize your model so you make fewer loans to people who will default on them.  \n",
    "\n",
    "- In this scenario a False Positive is an individual you would provide a load who would default, and a False Negative is someone not given a loan that would not default.   \n",
    "\n",
    "- In this imaginary company having a loan defaulted on is more detrimental to the business than missing a loan opportunity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe the inverse relationship between sensitivity and specificity.\n",
    "- Sensitivity : Of all the Class 1 observations, how many did we correctly identify?\n",
    "- Specificity : Of all the Non-Class 1 observations, how many did we correctly Identify?\n",
    "\n",
    "Typically, there is a general trade off and a gray area of observations that are difficult to classify as its a pretty even mixture of the two or more classes and thus nearly impossible to fully separate.  Maximizing sensitivity and correctly classifying all of the Class 1 observations typically results in having more false positives. \n",
    "\n",
    "Increasing FPs increases the denominator of Specificity and thus will reduce Specificity. This is bad! We want high Specificity!\n",
    "\n",
    "On the other hand, decreasing False Positives will decrease the denominator of Specificity and thus will increase the Specificity. This is good! We want high Specificity!\n",
    "\n",
    "However, these changes will be at the cost of increasing the number False Negatives (and thus decreasing the Sensitivity). Is this tradeoff acceptable to you? Depends on the problem you are trying to solve.\n",
    "\n",
    "## ROC AUC\n",
    "\n",
    "TPR vs FPR\n",
    "\n",
    "### Construct the ROC Curve.\n",
    "- The Receiver Operating Characteristic curve is used to show us the relationship between the True Positives and False Positives.\n",
    "- Typically we construct this curve to analyze how well a model performs and the optimal threshold. \n",
    "- You want to see a curve that has as much area as possible under it. Points closest to the top left corner are ideal. \n",
    "- You should set your threshold depending upon whether you want fewer False Positives or False Negatives. \n",
    "- A straight diagonal line from bottom left to top right essentially means your model is worthless. \n",
    "- You need to use the `predict_proba()` from a binary classifier model to construct a ROC. \n",
    "\n",
    "### Understand how AUC ROC is calculated and interpret AUC ROC.\n",
    "- Area Under the Curve. If your AUC = 1 your model is perfect, if AUC = 0.5 its worthless.  AUC is the area under a curve.\n",
    "\n",
    "### Define _imbalanced classes_ and discuss the implications.\n",
    "- Imbalanced classes are class distributions in which 1 or more class(es) significantly out-numbers the other(s). \n",
    "- Highly imbalanced classes make the accuracy score almost worthless .\n",
    "- In the event of a sample with 1000 observations: 990 are Class A and 10 are Class B. Predicting Class A every time, our baseline (null model), will result in a accuracy of 99%.  \n",
    "- When one class has significantly fewer observations than other classes, it makes it difficult for the model learn which features are predictive of that class.  \n",
    "\n",
    "\n",
    "### Describe methods for handling imbalanced classes.\n",
    "\n",
    "- Randomly reducing the number of observations of Class A = downsampling\n",
    "- Increasing the observations of Class B = upsampling - bootstrapping is doing this with replacement\n",
    "\n",
    "- Stratifying samples ensures the test set sees a relatively equal number of cases from all classes.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "### Define 'loss function'\n",
    "A loss function or objective function or cost function is the metric that our algorithm optimizes on. It tells us how well (or how badly) our model is performing. In the case of an ordinary least squares regression, the loss function is the sum of squared errors (SSE).\n",
    "\n",
    "### Define 'regularization'\n",
    "Regularization is the group of methods for constraining (regularizing) the coefficients of a linear model in order to reduce the error due to variance.\n",
    "\n",
    "### Least Squares Loss Function\n",
    "\n",
    "---\n",
    "\n",
    "Ordinary least squares regression minimizes the mean squared error (MSE) to fit the data:\n",
    "\n",
    "### $$ \\text{minimize:}\\; SSE(\\beta_0, \\beta_1, ...) = \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 $$\n",
    "\n",
    "Where our model predictions for $y$ are based on the sum of the $\\beta_0$ intercept and the products of $\\beta_i$ with $x_i$.\n",
    "\n",
    "### Defining the Lasso\n",
    "\n",
    "---\n",
    "\n",
    "Now we do the same thing as above but for the Lasso. You will be able to see how the coefficients change differently for both!  But first, let's define lasso.\n",
    "\n",
    "Lasso regression takes a different approach. Instead of adding the sum of squared $\\beta$ coefficients to the RSS, it adds the sum of the absolute values of the $\\beta$ coefficients:\n",
    "\n",
    "### $$ \\text{minimize:}\\; SSE + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\sum_{j=1}^p |\\beta_j|$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$|\\beta_j|$ is the absolute value of the $\\beta$ coefficient for variable $x_j$.\n",
    "\n",
    "$\\alpha$ is the strength of the regularization penalty component in the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "### Defining the Ridge\n",
    "\n",
    "---\n",
    "\n",
    "### $$ \\text{minimize:}\\; SSE+Ridge = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "**Where:**\n",
    "\n",
    "$\\beta_j^2$ is the squared coefficient for variable $x_j$.\n",
    "\n",
    "$\\sum_{j=1}^n \\beta_j^2$ is the sum of these squared coefficients for every variable in the model. This does **not** include the intercept $\\beta_0$.\n",
    "\n",
    "$\\alpha$ is a constant for the _strength_ of the regularization parameter. The higher the value, the greater the impact of this new component in the loss function. If the value was zero, we would revert back to just the least squares loss function. If the value was a billion, however, the residual sum of squares component would have a much smaller effect on the loss/cost than the regularization term.\n",
    "\n",
    "---\n",
    "\n",
    "### Defining the Elastic Net\n",
    "\n",
    "---\n",
    "\n",
    "The Elastic Net combines the Ridge and Lasso penalties.  It adds *both* penalties to the loss function:\n",
    "\n",
    "> \"[Elastic Net] allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge.\"\n",
    "-- sklearn docs\n",
    "\n",
    "#### $$ \\text{minimize:}\\; SSE + Ridge + Lasso = \\sum_{i=1}^n \\left(y_i - \\left(\\beta_0 + \\sum_{j=1}^p\\beta_j x_j\\right)\\right)^2 + \\alpha\\rho\\sum_{j=1}^p |\\beta_j| + \\alpha(1-\\rho)\\sum_{j=1}^p \\beta_j^2$$\n",
    "\n",
    "In the elastic net, the effect of the ridge versus the lasso is balanced by the $\\rho$ parameter.  It is the ratio of Lasso penalty to Ridge penalty and must be between zero and one.\n",
    "\n",
    "- **Ridge** is good at \"shrinking\" model coefficients.\n",
    "- **Lasso** is good at eliminating coefficients.\n",
    "- **ElasticNet** combines Ridge and Lasso.\n",
    "\n",
    "### Bottom line?\n",
    "\n",
    "In all cases, \"regularization strength\" is defined by a parameter $\\alpha$ (sometimes called $\\lambda$):\n",
    "- Increase $\\alpha$ (turn up regularization) \n",
    "    - Increase bias\n",
    "    - Decrease variance\n",
    "- Decrease $\\alpha$ (turn down regularization) \n",
    "    - Decrease bias\n",
    "    - Increase variance \n",
    "    \n",
    "---\n",
    "\n",
    "- The Ridge is best suited to deal with multicollinearity. \n",
    "- Lasso also deals with multicollinearity between variables, but in a more brutal way (it \"zeroes out\" the less effective variable).\n",
    "- The Lasso is particularly useful when you have redundant or unimportant variables. If you have 1000 variables in a dataset the Lasso can perform \"feature selection\" automatically for you by forcing coefficients to be zero.\n",
    "- Elastic Net combines both.\n",
    "\n",
    "### Regularization and the Bias-Variance Tradeoff\n",
    "We can think of regularization as exchanging error due to variance for error due to bias. Regularization does this by reducing or eliminating the coefficients of the independent variables, which reduces the complexity of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u5hSDUwCnM7G"
   },
   "source": [
    "##  Gridsearching, Hyperparameters and Pipelines\n",
    "\n",
    "### Describe what the terms gridsearch and hyperparameter mean.\n",
    "- Gridsearch is the act of searching a potentially multi-dimensional grid of possible values for the optimal combination.\n",
    "- Hyperparameters are values that can be adjusted in a model to have effects on how the model performs.\n",
    "\n",
    "### Build a gridsearching procedure from scratch.\n",
    "```python\n",
    "\n",
    "# for param1 in param1_list:\n",
    "#     for param2 in param2_list:\n",
    "#         for param3 in param3_list:\n",
    "#             Instantiate model with current params\n",
    "#             fit the model\n",
    "#             score the model\n",
    "#             store the scores and params list for each model\n",
    "           \n",
    "# search through Model_Score_Dict for max score \n",
    "# return Max score and ist associated params.\n",
    "```        \n",
    "\n",
    "### Apply sklearn's GridSearchCV object with basketball data to optimize a KNN model.\n",
    "\n",
    "\n",
    "### Use and evaluate attributes of the gridsearch object.\n",
    "\n",
    "\n",
    "### Describe the pitfalls of searching large hyperparameter spaces.\n",
    "- More hyper parameters = more combinations = more models = more time/computational expense\n",
    "- If you have a lot of data, model building takes time as well.\n",
    "- If you have a complex or deep model each model times more time to build as well.\n",
    "- Multiply this by however many Hyperparameters you're searching over.\n",
    "- You will want to use model based EDA to identify small hyperparameter spaces to search through instead of brute forcing and searching through ever value in existence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gqecqjR0yZ0L"
   },
   "source": [
    "### Pipeline Syntax\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t6CO7Nchyh5H"
   },
   "source": [
    "We set up a pipeline by passing a list of tuples in the format\n",
    "```\n",
    "('string_name', ClassObject())\n",
    "```\n",
    "Note that we can name our steps beforehand (each of the methods that we're using are a class in sklearn).\n",
    "```\n",
    "lasso = Lasso()\n",
    "('lasso', lasso)\n",
    "```\n",
    "\n",
    "We can include as many steps as we'd like. \n",
    "\n",
    "\n",
    "`make_pipeline` is an a convenience function to make a Pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZqtaQaLVzS6k"
   },
   "source": [
    "After we create/set up the pipeline, we `fit` it on our training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fmwvf__Mz1iN"
   },
   "source": [
    "Then we can `score` on our training set and testing set, generate predictions using `predict`, etc. - anything you would do with a \"regular\" model!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mpLqnj8QydqS"
   },
   "source": [
    "### GridSearch Syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SswTjsaI0CHa"
   },
   "source": [
    "`GridSearchCV` accepts a `Pipeline` object as an estimator and a param grid.\n",
    "\n",
    "The param grid uses the `string_name`s from your pipeline followed by a dunder `__` and the argument name for that particular step. You then provide an iterable to search over (generally a list or a range-style object)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example:  \n",
    "parameters = {'hyper_parameter__name of hyperparameter' : [different values for hyperparameter]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1pPWNNe1GwC"
   },
   "source": [
    "You can also specify the number of folds using `cv`. Default is 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUZCdrmt1OXj"
   },
   "source": [
    "We use this the same as other models, `fit`ting and `score`ing like normal (but now using the hyperparameters that gave us the best results)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FzHKqHd41hdE"
   },
   "source": [
    "Note that we'll use our `best_estimator_` to access the `Pipeline` that was fit with our `best_params_`.\n",
    "\n",
    "Within the `best_estimator_`there is a dictionary called `named_steps`. We can use our `string_names` to access the steps in our `Pipeline`. This is where we'll go to access info about the transformations and parameters done at each step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use and evaluate attributes of the gridsearch object.\n",
    "- A fit GridsearchCV object has the same methods as our normal fit models, and can be used with .predict and .score generate cross-validated scores and predictions on test data.  \n",
    "- The GridsearchCV object also has attributes that return the .best_params_, the .best_estimator_, and .best_score_ of the estimator chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```params = {\n",
    "      parmeter_1:[3, 5, 7]  \n",
    "      parmeter_2:[0.01, 0.1, 1.0]  \n",
    "      paramter_3:('x', 'y', 'z')\n",
    "}  ```\n",
    "We 3 values in each parameter, so it's $3x3x3=27$.  \n",
    "If we're using CV, than we multiply it by the number of folds (so CV=3 would be $3x3x3x3$) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Week4-LearningObjectives.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
