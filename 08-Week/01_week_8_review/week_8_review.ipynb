{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Unsupervised Learning\n",
    "**Define supervised and unsupervised learning**\n",
    "- Supervised learning has a known target (y) \n",
    "- y values are part of our training data\n",
    "- Unsupervised has no known target (no y)\n",
    "\n",
    "\n",
    "**Type of problems for unsupervised learning**\n",
    "- Clustering\n",
    "    - Creating profiles of customers or other things\n",
    "- PCA\n",
    "    - Focus on important information in features \n",
    "- Network Analysis\n",
    "    - Credit card fraud\n",
    "    - Connections on social media\n",
    "- Topic Modeling in NLP\n",
    "    \n",
    "- Can use unsupervised learning in a supervised learning pipeline\n",
    "\n",
    "\n",
    "# See amazing demonstrations [here](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reminder: \n",
    "### Don't forget to scale your data before doing K-means or DBSCAN\n",
    "\n",
    "These are distance-based algorithms. \n",
    "If a feature is on a different scale it can overwhelm other features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "**Understand basic unsupervised clustering problems**\n",
    "- Group similar points or groups together\n",
    "- Ride sharing \n",
    "- Find items with similar behavior (users, products, voters, etc)\n",
    "- Market segmentation\n",
    "- Understand complex systems\n",
    "- Discover meaningful categories for your data\n",
    "- Reduce the number of classes by grouping (e.g. bourbons, scotches -> whiskeys)\n",
    "- Feature reduction\n",
    "- Pre-processing! Create labels for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key terms**\n",
    "- K: Number of cluster\n",
    "- Means: mean points of the cluster\n",
    "- Centroid : center of cluster\n",
    "\n",
    "![clusters](images/kmeans.png)\n",
    "\n",
    "(source: global lecture)\n",
    "\n",
    "* Steps for K-Means Clustering**\n",
    "    1. Pick a value for k (the number of clusters to create).\n",
    "    1. Initialize k 'centroids' (starting points) in your data.\n",
    "    1. Create your clusters. Assign each point to the nearest centroid.\n",
    "    1. Make your clusters better. Move each centroid to the center of its cluster.\n",
    "    1. Repeat steps 3-4 until your centroids converge.\n",
    "\n",
    "\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Silhouette Score\n",
    "    - measure of how far apart clusters are\n",
    "    - high Silhouette = clusters are well separated\n",
    "    \n",
    "```\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "k = 5\n",
    "cl = KMeans(n_clusters=k)\n",
    "cl.fit(X)\n",
    "inertia = cl.inertia_\n",
    "sil = silhouette_score(X, cl.labels_)\n",
    "\n",
    "```\n",
    "    \n",
    "- Inertia\n",
    "    - sum of squared errors for each cluster\n",
    "    - compactness of a cluster (lower value)\n",
    "    \n",
    "**Picking k:**\n",
    "- Use prior knowledge!\n",
    "- Look at elbow in Silhouette or Inertia scree plot\n",
    "\n",
    "![image.png](images/scree_plot.png)\n",
    "\n",
    "Source: Images from global lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "**What is DBSCAN?**\n",
    "- Density based clustering\n",
    "\n",
    "**How does DBSCAN work?**\n",
    "1. Choose epsilon: defines a distance boundary from a point\n",
    "1. Choose min_num: the minimum number of points in the boundary\n",
    "1. Pick random start point\n",
    "1. Check if min number of points are in boundary\n",
    "    - If yes, new cluster, then moves to new random point in boundary\n",
    "    - If no, moves to new random point\n",
    "1. Stops when all points have been checked\n",
    "\n",
    "**How does DBSCAN compare to K-Means and Hierarchical Clustering?**\n",
    "    - K means needs a k selected, not necessary to choose nubmer of clusters in DBSCAN\n",
    "    - Reguired ensity of the observations can be set \n",
    "    - DBSCAN performs well with odd shaped clusters, less well when the clusters aren't clearly separated\n",
    "    \n",
    "**Implementation**\n",
    "\n",
    "```\n",
    "from sklearn.cluster import DBSCAN\n",
    "dbscan = DBSCAN(eps=2.3, min_samples=4)\n",
    "dbscan.fit(X_scaled)\n",
    "silhouette_score(X_scaled, dbscan.labels_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "\n",
    "- Rotate the data so the first component is along the X-axis. The second component is at 90 degrees (also known as orthogonal, perpendicular). And so on.\n",
    "\n",
    "**Differentiate between feature elimination and feature extraction.**\n",
    "- Feature Elimination = Dropping features \n",
    "- Feature Extraction = Combine existing features into new ones to reduce the number of features\n",
    "\n",
    "**Describe the PCA algorithm.**\n",
    "- Create a series of weights (eigenvectors) explaining the most variance in observations (PC1)\n",
    "- Along new principal component axis, creates new series of weights explaining most variance (PC2) , and so on. \n",
    "- $PC_x = W_i,jX +....W_nX_n $\n",
    "- Two assumption\n",
    "    - Large variance defines importance\n",
    "    - Linear relationship\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Use cases for PCA.**\n",
    "- Too many columns versus rows\n",
    "- High dimensionality\n",
    "- Addressing Multicollinearity problem in fitting model \n",
    "- Speed up training and prediction time\n",
    "- Reduce storage space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement PCA in scikit-learn.**\n",
    "1. ```Import PCA from sklearn.decomposition```\n",
    "1. Scale features (if necessary)\n",
    "1. Instantiate PCA\n",
    "1. Fit & transform\n",
    "1. Only include X because this is unsupervised, no y\n",
    "1. Review explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-18964d0f09d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecomposition\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m       \u001b[0;31m# PCA can take a number of components or a decimal for % of explained variance desired\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mZ_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()       \n",
    "Z_train = pca.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate and interpret proportion of explained variance.**\n",
    "- Explained variance: How much variance per component relative to the total variance of all observations.\n",
    "- ```pca.explained_variance_ratio_```\n",
    "- Cumulative variance: summing variance across all PC (PC1 + PC2 + â€¦.) until threshold is meet (80%,95%,etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `PCA(n_components)` argument can take a number of components desired or a decimal for % of cumulative variance desired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data\n",
    "\n",
    "### Types of missing data\n",
    "\n",
    "- MCAR (Missing Completely at Random)\n",
    "    - I'm a sleepy graduate student working in a lab, while pipetteing, I reach over to grab my pen but accidentally knock three petri dishes off the desk, from these petri dishes I lose all the data that I would have otherwise collected. \n",
    "        - The data of interest is not systematically different between respondents and nonrespondents.\n",
    "    \n",
    "- MAR (Missing at Random) \n",
    "    - I work in a lab that contains remote sensors. One sensor broke and thus did not gather information from 6:00 AM to 10:00 AM\n",
    "        - Conditional on data we have observed, the data of interest is not systematically different between respondents and nonrespondents.\n",
    "        - In this case, accounting for time can help account for the missingness!\n",
    "    \n",
    "    \n",
    "- NMAR (Not Missing at Random)\n",
    "    - I administer a survey that contains a question about income. Those who have lower incomes are less likely to respond about the question about income\n",
    "        - The data of interest are systematically different for respondents and nonrespondents.\n",
    "        - Whether or not an observation is missing depends on the value of the unobserved data itself!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommender Systems, Cosine Similarity, and Sparse Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Collaborative-based - Who else watched GOT and likes shows that are similar to GOT. Determined by ratings, usually.\n",
    "\n",
    "- Content-based - What is the content of GOT that's similar to other shows you're getting recommendations for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **cosine similarity** to determine which to recommend.\n",
    "\n",
    "![](https://neo4j.com/docs/graph-algorithms/current/images/cosine-similarity.png)\n",
    "\n",
    "Source: https://neo4j.com/docs/graph-algorithms/current/labs-algorithms/cosine/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://www.oreilly.com/library/view/mastering-machine-learning/9781785283451/assets/d258ae34-f4f8-4143-b3c2-0cb10f2b82de.png)\n",
    "\n",
    "Source: [Machine Learning Mastery book](https://www.oreilly.com/library/view/mastering-machine-learning/9781785283451/assets/d258ae34-f4f8-4143-b3c2-0cb10f2b82de.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cosine similarity is a measure of the angle between the vectors. \n",
    "\n",
    "- Euclidian and Manhattan (L1 and L2) are two distance-based metrics that can be applied to vectors.\n",
    "\n",
    "- Each column is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cold start problem** - difficult at first when you don't have many ratings from a new user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrix \n",
    "- makes it efficient to store lots of 0s\n",
    "- in contrast, the arrays you are used to seeing are **dense**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import pairwise_distances, cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out [this CrossValidated answer](https://stats.stackexchange.com/a/235676/198892) on z-score, cosine-similarity, and pearson correlation coefficient to help put a number of pieces together. ðŸ˜€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
