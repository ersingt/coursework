{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filling missing data with SimpleImputer, pipelines, and make_column_transformer\n",
    "\n",
    "_By Jeff Hale_\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson students will be able to:\n",
    "\n",
    "- Explain why you might want to use sklearn's `SimpleImputer`\n",
    "- Use `SimpleImputer` to fill missing values with the mean or a constant\n",
    "- Use `SimpleImputer` in a pipeline\n",
    "- Use `SimpleImptuter` with `make_column_transformer` and a pipeline to use different imputation strategies with different columns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleImputer is an sklearn class you can use so that you don't have to worry about X_test information leaking into your imputation metrics.\n",
    "\n",
    "It can be used in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Parameters: \n",
    " \n",
    "    missing_values : number, string, np.nan (default) or None\n",
    "        The placeholder for the missing values. All occurrences of\n",
    "        `missing_values` will be imputed.\n",
    "        \n",
    "    strategy : string, default='mean'\n",
    "        The imputation strategy.\n",
    "        - If \"mean\", then replace missing values using the mean along\n",
    "          each column. Can only be used with numeric data.\n",
    "        - If \"median\", then replace missing values using the median along\n",
    "          each column. Can only be used with numeric data.\n",
    "        - If \"most_frequent\", then replace missing using the most frequent\n",
    "          value along each column. Can be used with strings or numeric data.\n",
    "        - If \"constant\", then replace missing values with fill_value. Can be\n",
    "          used with strings or numeric data.\n",
    "      \n",
    "\n",
    "    fill_value : string or numerical value, default=None\n",
    "        When strategy == \"constant\", fill_value is used to replace all\n",
    "        occurrences of missing_values.\n",
    "        If left to the default, fill_value will be 0 when imputing numerical\n",
    "        data and \"missing_value\" for strings or object data types.\n",
    "\n",
    "    add_indicator : boolean, default=False\n",
    "        If True, a :class:`MissingIndicator` transform will stack onto output\n",
    "        of the imputer's transform. This allows a predictive estimator\n",
    "        to account for missingness despite imputation. If a feature has no\n",
    "        missing values at fit/train time, the feature won't appear on\n",
    "        the missing indicator even if there are missing values at\n",
    "        transform/test time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set things up with Titanic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ASLG-jUSzT5T"
   },
   "outputs": [],
   "source": [
    "# read in the data\n",
    "df_titanic = sns.load_dataset(\"Titanic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect \n",
    "df_titanic.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we just want to use `age` to predict survival. It has some missing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titanic.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# break into X and y\n",
    "X = df_titanic[['age']]\n",
    "y = df_titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use SimpleImputer in a pipeline \n",
    "\n",
    "The `.fit_transform()` method with compute the mean (the fit step) and then will fill the missing values in X_train's `age` column with the mean of X_train's existing `age` values (the train step). \n",
    "\n",
    "\n",
    "The `.transform()` method with `X_test` will use that same mean value computed just before with `.fit(X_train)` step.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate a SimpleImputer object that will use the mean to fill missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit it to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the transformed training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify no nulls in X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would we fill with most common value?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or instead, use a Pipeline and make life easier üòÅ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make a pipeline object \n",
    "\n",
    "Fill null values with the median using simple imputer and logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the pipeline on the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the pipeline on the train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score the pipeline on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could use grid search to hyperparameter tune the SimpleImputer and LogisticRegression arguments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use different imputation strategies on different columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use different imputation strategies on different columns, the easiest thing to do is use sklearn's `make_column_transformer`.\n",
    "\n",
    "## make_column_transformer\n",
    "\n",
    "`make_column_transformer` is a convenience method to make a `ColumnTransformer` object, similar to how `make_pipeline` is a convenience method to make a `Pipeline` object.\n",
    "\n",
    "A ColumnTransformer object can apply different transformers to different columns! üéâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters\n",
    "\n",
    "    *transformers : tuples\n",
    "        \n",
    "        Tuples of the form (transformer, column(s)) specifying the\n",
    "        transformer objects to be applied to subsets of the data.\n",
    "        \n",
    "        - transformer : estimator or {'passthrough', 'drop'}\n",
    "            Estimator must support :term:`fit` and :term:`transform`.\n",
    "            Special-cased strings 'drop' and 'passthrough' are accepted as\n",
    "            well, to indicate to drop the columns or to pass them through\n",
    "            untransformed, respectively.\n",
    "        \n",
    "        - column(s) : string or int, array-like of string or int, slice, boolean mask array or callable\n",
    "            Indexes the data on its second axis. Integers are interpreted as\n",
    "            positional columns, while strings can reference DataFrame columns\n",
    "            by name. A scalar string or int should be used where\n",
    "            ``transformer`` expects X to be a 1d array-like (vector),\n",
    "            otherwise a 2d array will be passed to the transformer.\n",
    "            A callable is passed the input data `X` and can return any of the\n",
    "            above.\n",
    "    \n",
    "    remainder : {'drop', 'passthrough'} or estimator, default 'drop'\n",
    "        By default, only the specified columns in `transformers` are\n",
    "        transformed and combined in the output, and the non-specified\n",
    "        columns are dropped. (default of ``'drop'``).\n",
    "        \n",
    "        By specifying ``remainder='passthrough'``, all remaining columns that\n",
    "        were not specified in `transformers` will be automatically passed\n",
    "        through. This subset of columns is concatenated with the output of\n",
    "        the transformers.\n",
    "        \n",
    "        By setting ``remainder`` to be an estimator, the remaining\n",
    "        non-specified columns will use the ``remainder`` estimator. The\n",
    "        estimator must support :term:`fit` and :term:`transform`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's use a larger set of columns for our X here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_titanic[['age', 'fare', 'embarked', 'sex']]\n",
    "y = df_titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a ColumnTransformer object with `make_column_transformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We could fit it, but we probably want to use it in a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ctf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass the ColumnTransformer object to the pipeline, dummify, and use logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we could pass the pipeline and a parameter grid to GridSearchCV and optimize the hyperparameters, but let's look at evaluation now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the performance\n",
    "\n",
    "look at the predictions and make a confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpack the confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "You've seen how to use `SimpleImputer` to fill missing values. You've seen how to use it with  `make_column_transformer` and pipelines.\n",
    "\n",
    "## Check for understanding\n",
    "\n",
    "- Why would you want to use `make_column_transformer` with pipelines ?\n",
    "- What are different imputation strategies?\n",
    "- What is precision?\n",
    "\n",
    "\n",
    "\n",
    "With `GridSearchCV`, `make_pipeline`, and `make_column_transformer` you can do preprocessing and grid searching efficiently!  üéâ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
