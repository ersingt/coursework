{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Recurrent Neural Network\n",
    "**Author**: Adi Bronshtein, DC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Background:** we're going to use a _Recurrent Neural Network_ for text classification. The key feature of RNNs is that the data loop back in the network. This gives RNNs a type of \"memory\" it can use to better understand sequential data. A popular choice type of RNN is the _Long Short-Term Memory_ (LSTM) network which allows for information to loop backwards in the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the regular imports \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import imdb # check out the dataset info at https://keras.io/datasets/\n",
    "from keras.models import Sequential # import the type of model we'll use\n",
    "from keras.layers import Dense, LSTM, Dropout # import the layers\n",
    "from keras.layers.embeddings import Embedding # import another kind of layers\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "# set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple LSTM for Sequence Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The IMDB Movie Review Dataset\n",
    "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words.\"\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word. \n",
    "\n",
    "Check out the full description and how to use the dataset and the `\"load_data()\"` method in the [Keras Documentation](https://keras.io/datasets/#imdb-movie-reviews-sentiment-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset On Movie Review Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the dataset using load_data, but only keep the top 5000 words. Other words would be 0. \n",
    "\n",
    "# imdb.load_data returns two tuples: (x_train, x_test) and (y_train, y_test). See the (link to) documentation above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorten/pad the input sequence - to make each observation have 500 features (you can change that value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View First Observation’s Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View First Observation’s Feature Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first observation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set the vector length\n",
    "\n",
    "# instantiate the neural network\n",
    "\n",
    "# first layer - Embedded layer with a length of 32 vectors (represent each word)\n",
    "\n",
    "# second layer - LSTM (long short-term memory) layer with 100 neurons\n",
    "\n",
    "# last layer - a fully densed (connected) layer with sigmoid activation function (binary classification)\n",
    "\n",
    "# compiling the network. Using binary crossentropy for log loss, adam as optimizer and accuracy as our metric\n",
    "\n",
    "# shows us all the model's informantion\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''fit the model and assign it to a history object (to get info from fitted model later)\n",
    "fitting on the trainig data and using the test data as validation/evaluation.\n",
    "when you have more time, try epochs=3 (or more!)''' \n",
    "# Train neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation of the model using the accuracy score \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Neural Network Performance History "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of time, we're only running one epoch during the lesson, so this visualization would be meaningless.   \n",
    "Try running this code later, when you have time, after using, say, 10 or 15 epochs and seeing the training and testing accurcy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test accuracy histories\n",
    "\n",
    "\n",
    "# Create count of the number of epochs\n",
    "\n",
    "# Visualize accuracy history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM For Sequence Classification With Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discussed earlier, RNNs like LSTM are pretty prone for overfitting. We can add a Dropout layer between the the Embedding and LSTM layers and the LSTM and Dense output layers. Each Dropout layer will drop a user-defined hyperparameter of units in the previous layer every batch. Remember in Keras the input layer is assumed to be the first layer and not added using the add. Therefore, if we want to add dropout to the input layer, the layer we add in our is a dropout layer. This layer contains both the proportion of the input layer’s units to drop 0.2 and input_shape defining the shape of the observation data. Next, after we add a dropout layer with 0.5 after each of the hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LSTM Neural Network Architecture (with Dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start neural network\n",
    "\n",
    "# Add a dropout layer for input layer\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "\n",
    "# Add fully connected layer with a ReLU activation function\n",
    "\n",
    "# Add a dropout layer for previous hidden layer\n",
    "\n",
    "# Add fully connected layer with a sigmoid activation function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile the Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train neural network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Neural Network Performance History "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of time, we're only running one epoch during the lesson, so this visualization would be meaningless.   \n",
    "Try running this code later, when you have time, after using, say, 10 or 15 epochs and seeing the training and testing accurcy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training and test accuracy histories\n",
    "\n",
    "\n",
    "# Create count of the number of epochs\n",
    "\n",
    "# Visualize accuracy history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
